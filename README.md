<p align="center">
  <h1 align="center">
  Awesome Visual-Language-Navigation (VLN)
  </h1>
</p>

This repository contains a curated list of resources addressing the VLN (Visual Language Navigation).
Additionally, it includes related papers from areas such as Learning-based Navigation, etc.

If you find some ignored papers, **feel free to [*create pull requests*](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM/blob/pdf/How-to-PR.md), or [*open issues*](https://github.com/KwanWaiPang/Awesome-VLN/issues/new)**. 

Contributions in any form to make this list more comprehensive are welcome.

If you find this repository useful, a simple star should be the best affirmation. ğŸ˜Š

Feel free to share this list with others!

# Overview
- [VLN](#VLN)
  - [Simulator and Dataset](#Simulator-and-Dataset)
  - [Survey Paper](#Survey-Paper)
- [Learning-based Navigation](#Learning-based-Navigation)
  - [Mapless navigation](#Mapless-navigation) 
- [Others](#Others)
  - [Occupancy Perception](#Occupancy-Perception)

# VLN
<!-- |---|`arXiv`<br>|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|-----------|
|2025|`arXiv`<br>HKUST-GZ|[PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory](https://arxiv.org/pdf/2511.06840v1)|---|Zero-shotç›®æ ‡å¯¼èˆªï¼Œæ— éœ€ prebuilt mapsï¼›å…¨æ™¯ï¼ˆ6ä¸ªæ–¹å‘ï¼‰RGBè¾“å…¥MLLMï¼ˆQwen2.5-VLï¼‰ï¼Œæ­¤å¤–ï¼Œç»“åˆå½“å‰çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ä»¥åŠå†å²è®°å¿†ä¿¡æ¯ï¼ˆå­˜å‚¨åœ¨åŠ¨æ€æœ‰ç•Œçš„è®°å¿†é˜Ÿåˆ—ï¼‰åˆ©ç”¨LLMï¼ˆDeepSeek-V3ï¼‰åšå¯¼èˆªå†³ç­–ï¼ˆå†³ç­–ç»“æœåŒ…æ‹¬å¯¼èˆªæ–¹å‘å’Œæ˜¯å¦æ‰¾åˆ°ç›®æ ‡çš„æ ‡å¿—ï¼‰ï¼Œåœ¨HM3Dæ•°æ®é›†ä¸ŠSRçº¦ä¸º43.5%|
|2025|`arXiv`<br>Peking University|[MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/pdf/2511.17889)|[![Github stars](https://img.shields.io/github/stars/AIGeeksGroup/MobileVLA-R1.svg)](https://github.com/AIGeeksGroup/MobileVLA-R1)<br>[website](https://aigeeksgroup.github.io/MobileVLA-R1/)<br>[Dataset](https://huggingface.co/datasets/AIGeeksGroup/MobileVLA-CoT)|ç»“æ„åŒ–çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰å®ç°è¯­ä¹‰æ¨ç†å’ŒæŒ‡ä»¤æ§åˆ¶å¯¹é½ï¼ˆGemini-2.5-Flashï¼Œé€šè¿‡æç¤ºè¯ï¼Œå°†RGB-Depthã€è¯­è¨€æŒ‡ä»¤ã€çŠ¶æ€åŠ¨ä½œå†å²ä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸ºæ§åˆ¶å‘½ä»¤ï¼‰ï¼›æå‡ºå¤šç²’åº¦çš„CoTå¯¹é½è¯­æ–™åº“ï¼šMobileVLA-CoTæ•°æ®é›†ï¼ŒåŒ…å«18Kä¸ªå‰§é›†çº§æ ·æœ¬ã€78Kä¸ªæ­¥éª¤çº§æ ·æœ¬å’Œä¸€ä¸ªåŒ…å«38K CoTæ³¨é‡Šçš„å¯¼èˆªå­é›†ï¼›é‡‡ç”¨CoTæ•°æ®é›†è¿›è¡Œç›‘ç£å¾®è°ƒå¯¹é½åŠRLè¿›è¡Œè®­ç»ƒï¼›MobileVLA-R1ä»¥NaVILAæ¶æ„ä¸ºåŸºç¡€ï¼Œå¼•å…¥æ·±åº¦å’Œç‚¹äº‘ç¼–ç å™¨ï¼Œå°†å¤šæ¨¡æ€ç‰¹å¾é€šè¿‡è½»é‡çº§æŠ•å½±æ¨¡å—èåˆï¼Œç„¶åä¸ LLaMA3-8B è¯­è¨€æ¨¡å‹å¯¹é½ï¼›è¯„ä¼°ä¸»è¦é‡‡ç”¨VLN-CEï¼ˆSRï¼š66~68%ï¼‰å’Œå››è¶³æœºå™¨äººçš„QUARDæ•°æ®é›†ï¼ˆå¹³å‡SR73%ï¼‰ï¼›çœŸæœºä¸ºUnitree Go2 ï¼ˆ L2 LiDARã€RGB-D ç›¸æœºå’ŒJetson Orin ï¼‰ |
|2025|`arXiv`<br>SRI International|[SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation](https://arxiv.org/pdf/2505.13729?)|---|åˆ©ç”¨LLMå®ç°å¼‚æ„å¤šæœºå™¨äººåä½œå¯¼èˆªï¼Œé¡¶å±‚å…¨å±€è§„åˆ’å™¨é‡‡ç”¨LLMç”Ÿæˆåä½œç­–ç•¥ï¼›ä¸­å±‚å±€éƒ¨è§„åˆ’å™¨åˆ©ç”¨ LLM ä¸ºæ¯ä¸ªæœºå™¨äººç”ŸæˆåŠ¨æ€çš„ã€é€æ­¥çš„è¡ŒåŠ¨è®¡åˆ’ï¼›åº•å±‚åŠ¨ä½œè§„åˆ’å™¨å°†ä¸­å±‚å±€éƒ¨è§„åˆ’å™¨ç”Ÿæˆçš„é€æ­¥è®¡åˆ’æ˜ å°„ä¸ºå…·ä½“çš„æœºå™¨äººåŠ¨ä½œåºåˆ—ï¼›graph generatorè´Ÿè´£åŸºäºæœºå™¨äººæ„ŸçŸ¥çš„ RGBD å›¾åƒæ„å»ºå’Œæ‰©å±•å±€éƒ¨ 3D åœºæ™¯å›¾|
|2025|`ICRA`<br>Chinese Academy of Sciences|[MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation](https://arxiv.org/pdf/2409.18800)|---|ç”¨è’¸é¦çš„æ–¹å¼ï¼ˆtwo-stage distillationï¼‰ï¼Œåœ¨ScaleVLNä»…1/7çš„å¤§å°ä¸‹ï¼Œè¾¾åˆ°äº†ç›¸åŒçš„å‡†ç¡®ç‡ï¼ˆREVERIEä¸Š>50%,R2Rä¸Š>70%ï¼‰|
|2025|`ICRA`<br> The University of Adelaide|[Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2502.19024)|---|ï¼ˆGVNavï¼‰ä¾é è°ƒå–ä½è§†è§’æœºå™¨äººï¼ˆæ¯”å¦‚æœºå™¨ç‹—ï¼Œæ‰«åœ°æœºå™¨äººç­‰ï¼‰åœ¨ä¸åŒé«˜åº¦ä¸‹è·å¾—çš„è§†è§‰ä¿¡æ¯ï¼Œæ¥è¡¥å…¨å½“å‰è§†è§’ï¼›é‡‡ç”¨çš„ScaleVLNï¼Œä»¿çœŸç¯å¢ƒæˆåŠŸç‡ä¸º50-60%ï¼Œæœºæ¢°ç‹—çœŸæœºæˆåŠŸç‡28-48%| 
|2025|`ICRA`<br>The University of Adelaide|[Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs](https://arxiv.org/pdf/2409.18794)|[![Github stars](https://img.shields.io/github/stars/YanyuanQiao/Open-Nav.svg)](https://github.com/YanyuanQiao/Open-Nav)<br>[website](https://sites.google.com/view/opennav)|åˆ©ç”¨å¼€æºçš„LLM(æœ¬åœ°éƒ¨ç½²éä¸ºcall API)å®ç°è¿ç»­ç¯å¢ƒä¸‹çš„zero-shot VLNï¼›Waypoint Prediction moduleè´Ÿè´£é¢„æµ‹æ½œåœ¨å¯å¯¼èˆªç‚¹ï¼Œåœºæ™¯æ„ŸçŸ¥æ¨¡å—åŒ…å«äº†RAMï¼ˆç‰©ä½“è¯†åˆ«ï¼‰å’ŒSpatialBotï¼ˆVLMæ¨¡å‹ï¼Œè¾“å…¥RGBå’Œæ·±åº¦ä¿¡æ¯ï¼Œåšç©ºé—´æ„ŸçŸ¥ï¼Œè¾“å‡ºä¸ºæ–‡æœ¬æè¿°ï¼‰ï¼›LLM Navigatoræ¥æ”¶å‰ä¸¤è€…çš„è¾“å…¥ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºï¼šæŒ‡ä»¤ç†è§£ã€è¿›åº¦ä¼°è®¡ã€å†³ç­–ï¼›çœŸæœºå®éªŒä¸ºè½®å¼æœºå™¨äººï¼ˆRTX 3080 GPUï¼‰ï¼›æµ‹è¯•äº†å››ä¸ªä¸åŒæ¨¡å‹ï¼ˆLlama3.1-70B-instruct, Qwen2-72Binstruct, Gemma2-27B-instruct and Phi3-14B-instructï¼‰æˆåŠŸç‡<20%|
|2025|`arXiv`<br>Adelaide University|[VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation](https://arxiv.org/pdf/2512.19021)|[![Github stars](https://img.shields.io/github/stars/william13077/IAmGoodNavigator.svg)](https://github.com/william13077/IAmGoodNavigator) <br>[website](https://sihaoevery.github.io/vlnverse/)|åŸºäº NVIDIA Isaac Sim æ„å»ºï¼ŒåŒ…å« 263 ä¸ªé«˜ä¿çœŸç‰©ç†åœºæ™¯ï¼Œå¹¶ä¸”ç»Ÿä¸€äº†ç»†ç²’åº¦ (Fine-grainedï¼Œä¸€æ­¥ä¸€æŒ‡ä»¤)ã€ç²—ç²’åº¦ (Coarse-grainedï¼Œç›®æ ‡å¯¼å‘)ã€è§†è§‰å‚è€ƒ (Visual-Referenceï¼Œçœ‹å›¾æ‰¾ç‰©)ã€é•¿ç¨‹ (Long-Horizonï¼Œå¤šé˜¶æ®µè¿ç»­å¯¼èˆª) åŠå¯¹è¯å¼ (Dialogue-basedï¼Œé€šè¿‡äº¤äº’è§£å†³æ­§ä¹‰) äº”å¤§å¯¼èˆªä»»åŠ¡ï¼›ä¸ä»…ä»…æ˜¯è§†è§‰ä¸Šçš„é«˜ä¿çœŸï¼Œæ¯ä¸ªåœºæ™¯ä¸­çš„ç‰©ä½“å‡å¯ç§»åŠ¨ï¼Œäº¤äº’ã€‚ç‰©ä½“çš„ç‰©ç†å±æ€§ï¼Œæ¯”å¦‚è´¨é‡ï¼Œæ‘©æ“¦ç³»æ•°ï¼Œåå…‰ç³»æ•°ä¹Ÿéƒ½æœ‰æä¾›ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æä¾›äº†è¯¦ç»†çš„æ‹“æ‰‘å’Œè¯­ä¹‰æ ‡æ³¨ï¼Œä»¥åŠoccupancy mapã€‚|
|2025|`arXiv`<br>Shanghai AI Laboratory|[Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation](https://arxiv.org/pdf/2512.08186)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternNav.svg)](https://github.com/InternRobotics/InternNav)<br>[website](https://internrobotics.github.io/internvla-n1-dualvln.github.io/)|ï¼ˆDualVLNï¼‰VLN é¢†åŸŸé¦–ä¸ªåŒç³»ç»ŸåŸºç¡€æ¨¡å‹ï¼›System2ï¼šåŸºäºQwen-VL-2.5(7B)çš„å…¨å±€è§„åˆ’å™¨ï¼Œä»¥çº¦ 2 Hz çš„é¢‘ç‡è¿è¡Œï¼Œè´Ÿè´£ç†è§£æŒ‡ä»¤ã€è§‚å¯Ÿç¯å¢ƒï¼Œå¹¶é¢„æµ‹åƒç´ çº§ç›®æ ‡ç‚¹ï¼ˆæ­¤å¤–ï¼ŒSystem2è¿˜å¯ä»¥è‡ªä¸»è°ƒæ•´è§†è§’ï¼‰ï¼›System1ï¼šè½»é‡çº§Diffusion Transformerï¼Œ30HZè¿è¡Œï¼Œæ¥æ”¶System2çš„åƒç´ ç›®æ ‡åŠéšå«çš„è¯­ä¹‰ç‰¹å¾ï¼Œç»“åˆå½“å‰é«˜é¢‘RGBå›¾åƒï¼Œç”Ÿæˆå¹³æ»‘ã€è¿ç»­ã€é¿éšœçš„è½¨è¿¹ï¼›çœŸæœºå®éªŒï¼šè½®å¼ï¼ˆTurtlebot4ï¼‰ã€å››è¶³ï¼ˆUnitree Go2ï¼‰ã€äººå½¢ï¼ˆUnitree G1ï¼‰ï¼Œå‡ä»…æ­è½½RealSense D455 å•ç›® RGB ç›¸æœº(æˆåŠŸç‡ï¼šR2R 64.3%ï¼ŒRxR 61.4%ï¼›ï¼‰|
|2025|`arXiv`<br>Peking University|[MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning](https://arxiv.org/pdf/2510.03142v1)|[website](https://pku-epic.github.io/MM-Nav-Web/)|è®­ç»ƒ3ä¸ªRLä¸“å®¶ï¼ˆåˆ°è¾¾/Reachingã€æŒ¤å‹/Squeezingã€èº²é¿/Avoidingï¼‰å¹¶è¿›è¡Œåˆå§‹VLAï¼ˆSigLIP+Qwen2-7Bï¼‰å¾®è°ƒï¼Œç„¶åéƒ¨ç½²åˆ°ä»¿çœŸç¯å¢ƒï¼Œå†è¿›è¡Œåœ¨çº¿çš„æ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒï¼Œéƒ¨ç½²åˆ°Unitree GO2ï¼ˆæ¨¡å‹è¿è¡Œåœ¨RTX5090ï¼‰ï¼Œ4ä¸ªé±¼çœ¼ç›¸æœºï¼ŒVKAè¾“å‡ºçš„é€Ÿåº¦ç”±åº•å±‚æ§åˆ¶å™¨æ‰§è¡Œï¼ˆå¹³å‡å“åº”ä¸º7HZï¼‰ï¼Œåˆ°è¾¾çš„æˆåŠŸç‡>80%ï¼Œæ··åˆä¸‰ç§æƒ…å†µæˆåŠŸç‡>47%|
|2025|ManifoldTechLtd|Odin-Nav-Stack|[![Github stars](https://img.shields.io/github/stars/ManifoldTechLtd/Odin-Nav-Stack.svg)](https://github.com/ManifoldTechLtd/Odin-Nav-Stack)<br>[website](https://manifoldtechltd.github.io/Odin-Nav-Stack-Webpage/)<br>[Blog](https://kwanwaipang.github.io/Odin-Navigation-Stack/)|ï¼ˆModular learning approachesï¼‰SLAM+åŠ¨æ€é¿éšœ/Neupan(åŸºäºé›·è¾¾ç‚¹è¾“å…¥çš„ç«¯åˆ°ç«¯å¯¼èˆªæ§åˆ¶)+è¯­ä¹‰å¯¼èˆª(åŸºäºæŒ‡ä»¤åˆ†è§£ä¸YoLoå®ç°ç‰©ä½“çš„æ£€æµ‹ã€å®šä½ä¸ç›¸å¯¹ç‰©ä½“çš„å¯¼èˆª)+åœºæ™¯æè¿°|
|2025|`arXiv`<br>The University of Hong Kong|[VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.17221)| [![Github stars](https://img.shields.io/github/stars/Qi-Zhangyang/GPT4Scene-and-VLN-R1.svg)](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1)<br>[website](https://vlnr1.github.io/)|åˆ©ç”¨å¤§å‹VLMï¼ˆQwen2-VLï¼‰å°†è§†é¢‘æ•°æ®æµè½¬æ¢ä¸ºè¿ç»­å¯¼èˆªactionï¼›Supervised fine-tuning(SFTï¼Œå®ç°action textçš„é¢„æµ‹)+Reinforcement fine-tuning (RFTï¼Œå¤šæ­¥é¢„æµ‹)ï¼›å¯¹äºè§†é¢‘è¾“å…¥é‡‡ç”¨Long-Short Memory Samplingç­–ç•¥ï¼Œå¹³è¡¡å†å²å¸§æ•°ä¸å®æ—¶æ€§ï¼ˆæ¨¡æ‹Ÿå™¨ä¸Šå¯¼èˆªæˆåŠŸç‡<35%ï¼‰|
|2025|`arXiv`<br>Horizon Robotics|[FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph](https://arxiv.org/pdf/2509.13733)| [![Github stars](https://img.shields.io/github/stars/HorizonRobotics/HoloAgent.svg)](https://github.com/HorizonRobotics/HoloAgent) <br>[website](https://horizonrobotics.github.io/robot_lab/fsr-vln/)|HMSGï¼šå¤šæ¨¡æ€åœ°å›¾è¡¨å¾ï¼ˆFAST-LIVO2ç­‰ï¼šå‡ ä½•+è¯­ä¹‰+æ˜¾å¼æ‹“æ‰‘å…³ç³»ï¼‰å®ç°ç²—ç•¥çš„room-levelå®šä½åˆ°ç²¾ç»†çš„ç›®æ ‡è§†è§’ä¸ç‰©ä½“å®šä½ï¼›æ¥ä¸‹æ¥FSRåŸºäºHMGSï¼Œå†åº”ç”¨VLMå®ç°æœ€ç»ˆç›®æ ‡çš„é€‰æ‹©ï¼›åŸºäºé€‰æ‹©çš„ç›®æ ‡å®ç°è·¯å¾„è§„åˆ’ä»¥åŠå…¨èº«æ§åˆ¶æ¥åˆ°è¾¾ç›®æ ‡ï¼›Unitree-G1éªŒè¯|
|2025|`arXiv`<br> University at Buffalo|[VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning](https://arxiv.org/pdf/2502.00931)|[website](https://sairlab.org/vlnav/)|é›†æˆäº†ç©ºé—´æ¨ç†ï¼ˆå°†è§†è§‰è¯­è¨€ç‰¹å¾è½¬æ¢ä¸ºç©ºé—´åˆ†æ•°åˆ†å¸ƒï¼‰ï¼Œåˆ†æ•°ï¼ˆCVL scoresï¼‰ä¸æ¯ä¸ªç›®æ ‡ç‚¹ç›¸ç»“åˆï¼Œå†é€šè¿‡curiosity-driven weightingå®ç°é€‰æ‹©ç›®æ ‡ç‚¹è¿›è¡Œæ¢ç´¢ï¼ˆä¿è¯æ‰€é€‰çš„ç›®æ ‡ç‚¹ä¸ä»…æ˜¯äººç±»æŒ‡ä»¤è¿˜æ˜¯è®©æœºå™¨äººæ¢ç´¢æœªçŸ¥åŒºåŸŸï¼‰,é€‰æ‹©ç›®æ ‡ç‚¹åé‡‡ç”¨ä¼ ç»Ÿçš„plannerè¿›è¡Œé¿éšœå¯¼èˆªï¼ˆModular learning approachesï¼‰ï¼›ç§»åŠ¨å°è½¦ï¼ˆOrin NXï¼‰ä¸Šå®ç°30HZé¢‘ç‡ï¼ˆæˆåŠŸç‡86.3%ï¼‰|
|2025|`arXiv`<br>Peking University|[Embodied navigation foundation model](https://arxiv.org/pdf/2509.12129)|[website](https://pku-epic.github.io/NavFoM-Web/)|(NavFoM)å…¨çƒé¦–ä¸ªè·¨æœ¬ä½“å…¨åŸŸç¯è§†å¯¼èˆªåŸºåº§å¤§æ¨¡å‹,æŠŠVLNã€Object-goal Navigationã€Visual Tracking å’ŒAutonomous Driving ç­‰ä¸åŒæœºå™¨äººçš„å¯¼èˆªä»»åŠ¡ç»Ÿä¸€åˆ°ç›¸åŒçš„èŒƒå¼,7Bå‚æ•°|
|2025|`arXiv`<br>Shanghai AI Laboratory|[InternVLA-N1: An Open Dual-System Vision-Language Navigation Foundation Model with Learned Latent Plans](https://internrobotics.github.io/internvla-n1.github.io/static/pdfs/InternVLA_N1.pdf)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternNav.svg)](https://github.com/InternRobotics/InternNav)<br> [website](https://internrobotics.github.io/internvla-n1.github.io/) |é¦–ä¸ªå¼€æºçš„åŸºäºåŒç³»ç»ŸVLNæ¨¡å‹:System 2 ç”¨äºæ‰§è¡Œå¤šè½®çš„åŸºäºè¯­è¨€æŒ‡ä»¤ã€è§‚æµ‹ï¼ˆè§†è§’æ„ŸçŸ¥ï¼‰çš„ç²¾ç¡®è§„åˆ’ã€‚è€ŒSystem1åˆ™æ˜¯è´Ÿè´£åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸‹ï¼Œæ‰§è¡ŒSystem2è¾“å‡ºçš„è§„åˆ’ã€‚å…¶ä¸­ï¼Œå­¦ä¹ åˆ°çš„latent plansä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæ¥å®ç°ä¸¤ä¸ªç³»ç»Ÿä¹‹é—´çš„äº¤äº’;æ­¤å¤–ï¼Œæå‡ºInternData-N1å¯¼èˆªæ•°æ®é›†ï¼ŒåŒ…å«äº†è¶…è¿‡3000ä¸ªåœºæ™¯çš„50 millionçš„å›¾åƒï¼Œä¸€å…±4,839å…¬é‡Œ|
|2025|`arXiv`|[Odyssey: Open-world quadrupeds exploration and manipulation for long-horizon tasks](https://arxiv.org/pdf/2508.08240)|[website](https://kaijwang.github.io/odyssey.github.io/)|---|
|2025|`arXiv`|[OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/pdf/2511.06182)|---|---|
|2025|`arXiv`<br>University of Texas at Austin|[VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/pdf/2509.18592?)|[![Github stars](https://img.shields.io/github/stars/VLN-Zero/vln-zero.github.io.svg)](https://github.com/VLN-Zero/vln-zero.github.io)<br>[website](https://vln-zero.github.io/)|é€šè¿‡VLMæ„å»ºsymbolic scene graphsï¼ˆç¬¦å·åœºæ™¯å›¾ï¼‰ï¼Œè¿›è€Œå¯ä»¥å®ç°zero-shotçš„neurosymbolic navigationï¼ˆç¥ç»ç¬¦å·å¯¼èˆªï¼‰|
|2025|`arXiv`<br> Alibaba Group|[JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](https://arxiv.org/pdf/2509.22548)|[![Github stars](https://img.shields.io/github/stars/MIV-XJTU/JanusVLN.svg)](https://github.com/MIV-XJTU/JanusVLN)<br> [Website](https://miv-xjtu.github.io/JanusVLN.github.io/)|é€šè¿‡ä¸€ä¸ª3Dè§†è§‰åŸºç¡€æ¨¡å‹æ¥æ‰©å±•MLLM,å®ç°ä»ç©ºé—´å‡ ä½•ç¼–ç å™¨ä¸­è·å–3Då…ˆéªŒçŸ¥è¯†,è¿›è€Œå¢å¼ºæ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›;åŒéšå¼è®°å¿†(ç©ºé—´å‡ ä½•ä¸è§†è§‰è¯­ä¹‰è®°å¿†)çš„å†å²é”®å€¼å¯¹åˆ™æ˜¯é€šè¿‡3Dç©ºé—´å‡ ä½•ç¼–ç å™¨å’ŒMLLMçš„è¯­ä¹‰è§†è§‰ç¼–ç å™¨æ¥åˆ†åˆ«æå–;é€šè¿‡æ»‘åŠ¨çª—å£è¿›è¡ŒåŠ¨æ€åŠå¢é‡å¼æ›´æ–°|
|2025|`arXiv`<br>Shanghai AI Laboratory|[Streamvln: Streaming vision-and-language navigation via slowfast context modeling](https://arxiv.org/pdf/2507.05240)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/StreamVLN.svg)](https://github.com/InternRobotics/StreamVLN)<br> [Website](https://streamvln.github.io/)|å°†Video-LLMï¼ˆLLaVA-Videoæ¨¡å‹ï¼Œé‡‡ç”¨çš„æ˜¯Qwen2-7Bï¼‰æ‰©å±•ä¸ºäº¤é”™çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œè¿›è€Œå®ç°å¤šè½®å¯¹è¯ä¸‹ä¸è§†é¢‘çš„è¿ç»­äº¤äº’;ä¸ºäº†åº”å¯¹é•¿æœŸä¸Šä¸‹æ–‡ç®¡ç†å’Œè®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜ï¼ŒStreamVLNé‡‡ç”¨æ··åˆçš„æ…¢é€Ÿ-å¿«é€Ÿä¸Šä¸‹æ–‡å»ºæ¨¡ç­–ç•¥:å¿«é€Ÿæµå¼å¯¹è¯éƒ¨åˆ†é€šè¿‡æ´»åŠ¨å¯¹è¯çš„æ»‘åŠ¨çª—å£ä¿ƒè¿›å“åº”å¼åŠ¨ä½œç”Ÿæˆ;ç¼“æ…¢æ›´æ–°çš„å†…å­˜éƒ¨åˆ†ä½¿ç”¨3Dæ„ŸçŸ¥Tokenæ¥ä¿®å‰ªç­–ç•¥ä»¥åŠå‹ç¼©å†å²è§†è§‰çŠ¶æ€ã€‚è¿™éƒ¨åˆ†ä½¿å¾—StreamVLNå¯ä»¥é€šè¿‡é”®å€¼å¯¹çš„ç¼“å­˜é‡ç”¨ï¼Œå®ç°è¿è´¯çš„å¤šå›åˆå¯¹è¯|
|2025|`arXiv`|[GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/pdf/2509.10454)|[![Github stars](https://img.shields.io/github/stars/bagh2178/GC-VLN.svg)](https://github.com/bagh2178/GC-VLN)<br>[Website](https://bagh2178.github.io/GC-VLN/)|---|
|2025|`arXiv`|[Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting](https://arxiv.org/pdf/2509.20499)|---|---|
|2025|`arXiv`<br> HKUST-GZ|[SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning](https://arxiv.org/pdf/2509.20739)|---|ä¸ä¾èµ–äºSLAMçš„å¯¼èˆªæ¡†æ¶ã€‚é€šè¿‡VLMæ¥å®ç°åœºæ™¯ï¼ˆQwenï¼‰ä»¥åŠç‰©ä½“çº§åˆ«ï¼ˆGrounding DINOï¼‰çš„è¯­ä¹‰æ¨ç†,è¿›è€Œæ„å»ºè½»é‡çš„æ‹“æ‰‘åœ°å›¾ï¼ˆtopological representationsï¼‰ã€‚åŸºäºLLMï¼ˆGPT-4ï¼‰çš„å®ç°å­ç›®æ ‡çš„é€‰æ‹©ï¼Œè€ŒåŸºäºè§†è§‰çš„å±€éƒ¨è§„åˆ’ï¼ˆViplannerï¼‰å®ç°éšœç¢ç‰©èº²é¿ã€‚ æœ€åå†é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å®ç°è…¿å¼æœºå™¨äººçš„è¿åŠ¨æ§åˆ¶ã€‚|
|2025|`arXiv`<br>Robotics and AI Institute|[Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation](https://arxiv.org/pdf/2411.07848)|[website](https://sonia-raychaudhuri.github.io/nlslam/)<br>[Reading](https://kwanwaipang.github.io/nl-slam/)|LLMæ„å»ºè¯­è¨€å…ˆéªŒå› å­å›¾ä¸ä¼ ç»Ÿçš„SLAMå¯¼èˆªå› å­å›¾ç›¸ç»“åˆ| 
|2025|`RSS`<br>Peking University|[Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks](https://arxiv.org/pdf/2412.06224)|[![Github stars](https://img.shields.io/github/stars/jzhzhang/Uni-NaVid.svg)](https://github.com/jzhzhang/Uni-NaVid)<br>[website](https://pku-epic.github.io/Uni-NaVid/)|NaVidçš„æ‰©å±•ç‰ˆ,æ·»åŠ äº†tokenèåˆæœºåˆ¶å®ç°åŠ é€Ÿä»¥åŠæ‹“å±•VLNä»»åŠ¡ä¸º4ä¸ªå­ä»»åŠ¡(è§†è§‰è¯­è¨€å¯¼èˆªã€ç›®æ ‡å¯¼èˆªã€å…·èº«é—®ç­”ã€äººå‘˜è·Ÿéš)ï¼›æ¡†æ¶ä¸ºï¼šè§†è§‰ç¼–ç å™¨ï¼Œonline token merge mechanism ä»¥åŠLLMï¼ˆå…¸å‹çš„å°†è§†è§‰è§‚æµ‹ç¼–ç ä¸ºåºåˆ—è·Ÿè¯­è¨€tokenä¸€èµ·è¾“å…¥åˆ°LLMçš„æ¡†æ¶ï¼‰|
|2025|`RSS`<br>UC San Diego|[NaVILA: Legged Robot Vision-Language-Action Model for Navigation](https://arxiv.org/pdf/2412.04453)|[![Github stars](https://img.shields.io/github/stars/AnjieCheng/NaVILA.svg)](https://github.com/AnjieCheng/NaVILA)<br>[website](https://navila-bot.github.io/)<br>[Test](https://kwanwaipang.github.io/NaVILA/)|é‡‡ç”¨åŒç³»ç»Ÿæ¶æ„ï¼šVLM(è§†è§‰è¯­è¨€æ¨¡å‹)+locomotion policy (RLå®ç°çš„åŸºäºè§†è§‰çš„è¿åŠ¨æ§åˆ¶)ã€‚VLMå°†è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºmid-level action/åŠ¨ä½œè¯­è¨€æŒ‡ä»¤ï¼Œå†ç”±locomotion policyç¿»è¯‘ä¸ºæœºå™¨äººlow-level action/è¿æ§ç”µæœºæŒ‡ä»¤ï¼›|
|2025|`ICCV`|[Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation](https://arxiv.org/pdf/2507.04047)|[![Github stars](https://img.shields.io/github/stars/MTU3D/MTU3D.svg)](https://github.com/MTU3D/MTU3D)<br>[website](https://mtu3d.github.io/)|---| 
| 2025 | `ACL` | [MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation](https://arxiv.org/pdf/2502.13451) |---|---|
| 2025 | `CVPR` | [Scene Map-based Prompt Tuning for Navigation Instruction Generation](https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf) |---|---|
| 2025 | `ACL` | [NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM](https://arxiv.org/pdf/2502.11142) | [![Github stars](https://img.shields.io/github/stars/MrZihan/NavRAG.svg)](https://github.com/MrZihan/NavRAG) |---|
| 2025 | ICLR | [Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel](https://arxiv.org/abs/2412.08467) | [![Github stars](https://img.shields.io/github/stars/wz0919/VLN-SRDF.svg)](https://github.com/wz0919/VLN-SRDF) |---|
| 2025 | `ICCV` | [SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](https://arxiv.org/pdf/2412.05552) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/SAME.svg)](https://github.com/GengzeZhou/SAME) |---|
| 2025 | `ICCV` | [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2506.23468) | [![Github stars](https://img.shields.io/github/stars/Feliciaxyao/NavMorph.svg)](https://github.com/Feliciaxyao/NavMorph) |---|
| 2025 | `AAAI` | [Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation](https://arxiv.org/abs/2407.05890) |---|---|
| 2025 | Arxiv | [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/pdf/2506.01551) | [![Github stars](https://img.shields.io/github/stars/expectorlin/EvolveNav.svg)](https://github.com/expectorlin/EvolveNav) |---|
| 2025 | `CVPR` | [Do Visual Imaginations Improve Vision-and-Language Navigation Agents?](https://arxiv.org/pdf/2503.16394) |---|---|
|2024|`arXiv`<br>New York University|[Orionnav: Online planning for robot autonomy with context-aware llm and open-vocabulary semantic scene graphs](https://arxiv.org/pdf/2410.06239)|[video](https://www.youtube.com/watch?v=gqWCSjLLG1E)|åœ¨çº¿è‡ªä¸»å¯¼èˆªæ¡†æ¶ï¼ˆå››è¶³æœºå™¨äººã€Jetson AGX Orinå’ŒJetson Orin Nanoã€ä¼ æ„Ÿå™¨ï¼šRGBDã€LiDARã€IMUã€è…¿éƒ¨é‡Œç¨‹è®¡ï¼‰ï¼šLiDAR-SLAMï¼ˆ2Dæ …æ ¼åœ°å›¾ä¸ºä¸»ï¼‰+å¼€æ”¾è¯æ±‡è¡¨3Dè¯­ä¹‰æ˜ å°„æ–¹æ³•ï¼ˆåŸºäºRGBDçš„è¯­ä¹‰ç‰©ä½“åœ°å›¾ï¼ŒFC-CLIPï¼‰+åŸºäºLLMï¼ˆGPT-4-Turbo,äº‘ç«¯APIè°ƒç”¨ï¼‰çš„è§„åˆ’å™¨+ROS2å¯¼èˆªstack+[mexplore ROS2](https://github.com/robo-friends/m-explore-ros2)æ¢ç´¢ï¼›96æ¬¡çœŸæœºå®éªŒä¸­æˆåŠŸå®Œæˆäº†85æ¬¡ï¼ŒæˆåŠŸç‡ä¸º88.5%|
| 2024 | `AAAI` | [VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2402.03561) |---|---|
| 2024 | `CVPR` | [Volumetric Environment Representation for Vision-Language Navigation](https://arxiv.org/pdf/2403.14158) | [![Github stars](https://img.shields.io/github/stars/DefaultRui/VLN-VER.svg)](https://github.com/DefaultRui/VLN-VER) |---|
|2024|`ECCV`<br>University of Adelaid|[Navgpt-2: Unleashing navigational reasoning capability for large vision-language models](https://arxiv.org/pdf/2407.12366?)|[![Github stars](https://img.shields.io/github/stars/GengzeZhou/NavGPT-2.svg)](https://github.com/GengzeZhou/NavGPT-2)|VLMå¤„ç†å›¾åƒå’ŒæŒ‡ä»¤ï¼Œç„¶åå°†VLMè·å–çš„è§†è§‰Tokenä½œä¸ºLLMï¼ˆFrozenï¼‰çš„è¾“å…¥ï¼Œç”±LLMç”Ÿæˆå¯¼èˆªæ¨ç†ï¼ŒLLMçš„æ¨ç†åˆ™è¿›ä¸€æ­¥è¾“å…¥åˆ°å¯¼èˆªç­–ç•¥ç½‘ç»œè¾“å‡ºaction| 
| 2024 | `CVPR` | [Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation](https://arxiv.org/pdf/2404.01943) | [![Github stars](https://img.shields.io/github/stars/MrZihan/HNR-VLN.svg)](https://github.com/MrZihan/HNR-VLN) |---|
| 2024 | `TPAMI` | [ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2304.03047v2) | [![Github stars](https://img.shields.io/github/stars/MarSaKi/ETPNav.svg)](https://github.com/MarSaKi/ETPNav) |---|
| 2024 | `MM` | [Narrowing the Gap between Vision and Action in Navigation](https://www.arxiv.org/abs/2408.10388) |---|---|
| 2024 | `ECCV` | [LLM as Copilot for Coarse-grained Vision-and-Language Navigation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf) |---|---|
| 2024 | `ICRA` | [Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions](https://ieeexplore.ieee.org/abstract/document/10611565) | [![Github stars](https://img.shields.io/github/stars/LYX0501/DiscussNav.svg)](https://github.com/LYX0501/DiscussNav) |---|
| 2024 | `ACL` | [MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation](https://arxiv.org/abs/2401.07314) | [![Github stars](https://img.shields.io/github/stars/chen-judge/MapGPT.svg)](https://chen-judge.github.io/MapGPT/) |---|
| 2024 |`arXiv`| [MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains](https://arxiv.org/pdf/2405.10620) |---|---|
| 2024 |`arXiv`| [InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment](https://arxiv.org/pdf/2406.04882) | [![Github stars](https://img.shields.io/github/stars/LYX0501/InstructNav.svg)](https://github.com/LYX0501/InstructNav) |---|
| 2024 | `AAAI` | [NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models](https://arxiv.org/abs/2305.16986) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/NavGPT.svg)](https://github.com/GengzeZhou/NavGPT) |é‡‡ç”¨GPT-4ä½œä¸ºzero-shotå¯¼èˆªå™¨ï¼Œå°†è§†è§‰è§‚æµ‹è½¬æ¢ä¸ºæ–‡æœ¬æè¿°çš„å€™é€‰viewpointï¼Œç„¶åç”¨LLMå¤„ç†è¿›ä¸€æ­¥ç”Ÿæˆæ–‡æœ¬action|
| 2024 | NACCL Findings | [LangNav: Language as a Perceptual Representation for Navigation](https://aclanthology.org/2024.findings-naacl.60.pdf) | [![Github stars](https://img.shields.io/github/stars/pbw-Berwin/LangNav.svg)](https://github.com/pbw-Berwin/LangNav) |---|
| 2024 |`arXiv`| [NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning](https://arxiv.org/abs/2403.07376) | [![Github stars](https://img.shields.io/github/stars/expectorlin/NavCoT.svg)](https://github.com/expectorlin/NavCoT) |---|
| 2024 | `CVPR` | [Towards Learning a Generalist Model for Embodied Navigation](https://arxiv.org/abs/2312.02010) | [![Github stars](https://img.shields.io/github/stars/LaVi-Lab/NaviLLM.svg)](https://github.com/LaVi-Lab/NaviLLM) |NaviLLM|
| 2024 | `RSS`<br> Peking University | [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation](https://arxiv.org/pdf/2402.15852) | [![Github stars](https://img.shields.io/github/stars/jzhzhang/NaVid-VLN-CE.svg)](https://github.com/jzhzhang/NaVid-VLN-CE)<br>[website](https://pku-epic.github.io/NaVid/) |é¦–ä¸ªæ¢ç´¢VLMï¼ˆLLaMA-VIDï¼‰æ˜¯å¦å¯ä»¥èƒœä»»VLNä»»åŠ¡ï¼Œæå‡ºç¬¬ä¸€ä¸ªvideo VLM-based navigation agentï¼›åœ¨è¿ç»­ç©ºé—´ä¸‹ç›´æ¥æ¨æ–­low-level action|
| 2024 |`EMNLP`| [Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation](https://arxiv.org/pdf/2409.17313) | [![Github stars](https://img.shields.io/github/stars/zehao-wang/navnuances.svg)](https://github.com/zehao-wang/navnuances) |---|
| 2023 | `CVPR` | [Behavioral Analysis of Vision-and-Language Navigation Agents](https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf) | [![Github stars](https://img.shields.io/github/stars/Yoark/vln-behave.svg)](https://github.com/Yoark/vln-behave) |---|
| 2023 | `ICCV` | [March in Chat: Interactive Prompting for Remote Embodied Referring Expression](https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YanyuanQiao/MiC.svg)](https://github.com/YanyuanQiao/MiC) |---|
| 2023 |`arXiv`| [Vision and Language Navigation in the Real World via Online Visual Language Mapping](https://arxiv.org/pdf/2310.10822) |---|---|
| 2023 | `NeurIPS` | [A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models](https://peihaochen.github.io/files/publications/A2Nav.pdf) |---|---|
| 2023 | `ICCV` | [BEVBert: Multimodal Map Pre-training for Language-guided Navigation](https://arxiv.org/pdf/2212.04385) | [![Github stars](https://img.shields.io/github/stars/MarSaKi/VLN-BEVBert.svg)](https://github.com/MarSaKi/VLN-BEVBert) |---|
|2023|`CVPR`<br>University of Washington|[Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/real-stanford/cow.svg)](https://github.com/real-stanford/cow)<br>[website](https://cow.cs.columbia.edu/)|CLIP on Wheelsï¼Œå°†CLIPç”¨åˆ°VLNä»»åŠ¡ä¸­|
|2023|`NIPS`<br>Chinese Academy of Sciences|[Frequency-enhanced data augmentation for vision-and-language navigation](https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf)|[![Github stars](https://img.shields.io/github/stars/hekj/FDA.svg)](https://github.com/hekj/FDA)|å°†VLNçš„æ•°æ®å¢å¹¿é—®é¢˜ä»spatial domainçš„æ¢ç´¢è½¬ç§»åˆ°Fourier domain|
|2023|`NIPS`<br>Peking University|[Find what you want: Learning demand-conditioned object attribute space for demand-driven navigation](https://proceedings.neurips.cc/paper_files/paper/2023/file/34e278fbbd7d6d7d788c98065988e1a9-Paper-Conference.pdf)|[![Github stars](https://img.shields.io/github/stars/whcpumpkin/Demand-driven-navigation.svg)](https://github.com/whcpumpkin/Demand-driven-navigation)<br>[website](https://sites.google.com/view/demand-driven-navigation)|é¦–æ¬¡æå‡ºæå‡ºéœ€æ±‚é©±åŠ¨çš„å¯¼èˆªä»»åŠ¡ï¼ˆDDNï¼‰;é€šè¿‡ä»LLMï¼ˆGPT-3ï¼‰ä¸­æå–å¸¸è¯†çŸ¥è¯†æ¥è·å–å¯¹è±¡æ–‡æœ¬å±æ€§ç‰¹å¾ã€‚è€Œæ–‡æœ¬å±æ€§ç‰¹å¾è·Ÿè§†è§‰ç‰¹å¾é€šè¿‡CLIPæ¥å¯¹é½|
|2023|`ACL`<br>University of California|[Aerial vision-and-dialog navigation](https://arxiv.org/pdf/2205.12219)|[![Github stars](https://img.shields.io/github/stars/eric-ai-lab/Aerial-Vision-and-Dialog-Navigation.svg)](https://github.com/eric-ai-lab/Aerial-Vision-and-Dialog-Navigation)<br>[website](https://sites.google.com/view/aerial-vision-and-dialog/home)|ç±»ä¼¼äºCVDNçš„åŸºäºäººç±»å¯¹è¯çš„æ— äººæœºå¯¼èˆª| 
| 2023 | `AAAI` | [Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation](https://arxiv.org/pdf/2302.06072) |---|---|
| 2023 | `ICCV` | [Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation](https://arxiv.org/abs/2308.12587) | [![Github stars](https://img.shields.io/github/stars/CSir1996/VLN-GELA.svg)](https://github.com/CSir1996/VLN-GELA) |---|
| 2023 | `CVPR` | [Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/chengaopro/AZHP.svg)](https://github.com/chengaopro/AZHP) |---|
| 2023 | `ICCV` | [Bird's-Eye-View Scene Graph for Vision-Language Navigation](https://arxiv.org/abs/2308.04758) |---|---|
| 2023 |`EMNLP`| [Masked Path Modeling for Vision-and-Language Navigation](https://arxiv.org/abs/2305.14268) |---|---|
| 2023 | `CVPR` | [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics](https://arxiv.org/pdf/2304.04907) | [![Github stars](https://img.shields.io/github/stars/jialuli-luka/VLN-SIG.svg)](https://github.com/jialuli-luka/VLN-SIG) |---|
| 2023 | `TPAMI` | [HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation](https://ieeexplore.ieee.org/document/10006384) |---|---|
| 2023 | `TPAMI` | [Learning to Follow and Generate Instructions for Language-Capable Navigation](https://ieeexplore.ieee.org/document/10359152) |---|---|
| 2023 | `CVPR` | [A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning](https://arxiv.org/pdf/2210.03112) |[Dataset](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5)|---|
| 2023 | `CVPR` | [Lana: A Language-Capable Navigator for Instruction Following and Generation](https://arxiv.org/abs/2303.08409) | [![Github stars](https://img.shields.io/github/stars/wxh1996/LANA-VLN.svg)](https://github.com/wxh1996/LANA-VLN) |---|
| 2023 | `CVPR` | [KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/xiangyangli-cn/KERM.svg)](https://github.com/xiangyangli-cn/KERM) |---|
| 2023 | `MM` | [PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation](https://arxiv.org/pdf/2305.11918) |---|---|
| 2023 |`arXiv`| [CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation](https://arxiv.org/abs/2103.00852) |---|---|
| 2023 | `ACL` | [VLN-Trans: Translator for the Vision and Language Navigation Agent](https://arxiv.org/pdf/2302.09230) | [![Github stars](https://img.shields.io/github/stars/HLR/VLN-trans.svg)](https://github.com/HLR/VLN-trans) |---|
| 2022 | `ACL` | [Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration](https://arxiv.org/pdf/2203.04006) | [![Github stars](https://img.shields.io/github/stars/liangcici/Probes-VLN.svg)](https://github.com/liangcici/Probes-VLN) |---|
| 2022 | `CVPR` | [Less is More: Generating Grounded Navigation Instructions from Landmarks](https://arxiv.org/pdf/2004.14973) | [![Github stars](https://img.shields.io/github/stars/google-research-datasets/RxR.svg)](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5) |---|
| 2022 | `MM` | [Target-Driven Structured Transformer Planner for Vision-Language Navigation](https://arxiv.org/pdf/2207.11201) | [![Github stars](https://img.shields.io/github/stars/YushengZhao/TD-STP.svg)](https://github.com/YushengZhao/TD-STP) |---|
| 2022 | `CVPR` | [HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation](https://ieeexplore.ieee.org/document/9880046) | [![Github stars](https://img.shields.io/github/stars/YanyuanQiao/HOP-VLN.svg)](https://github.com/YanyuanQiao/HOP-VLN) |---|
| 2022 | `International Conference on Computational Linguistics` | [LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation](https://aclanthology.org/2022.coling-1.505.pdf) | [![Github stars](https://img.shields.io/github/stars/HLR/LOViS.svg)](https://github.com/HLR/LOViS) |---|
| 2022 | NACCL | [Diagnosing Vision-and-Language Navigation: What Really Matters](https://aclanthology.org/2022.naacl-main.438.pdf) | [![Github stars](https://img.shields.io/github/stars/VegB/Diagnose_VLN.svg)](https://github.com/VegB/Diagnose_VLN) |---|
| 2022 |`arXiv`<br>University of Maryland| [CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/pdf/2211.16649) |---|é‡‡ç”¨å±€éƒ¨å¢é‡è§„åˆ’æ¡†æ¶ï¼Œå°†å¯¼èˆªè¿‡ç¨‹åˆ†è§£ä¸ºå•æ­¥å†³ç­–åºåˆ—(æŒ‡ä»¤åˆ†è§£)ï¼šé€šè¿‡CLIPå®æ—¶è®¡ç®—è§†è§‰åœºæ™¯ä¸å­æŒ‡ä»¤ï¼ˆå¦‚ã€Œæ‰¾å¨æˆ¿é—¨ã€ï¼‰çš„åŒ¹é…åº¦ï¼Œç›´æ¥é¢„æµ‹æœ€ä¼˜åŠ¨ä½œï¼ˆå·¦è½¬/ç›´è¡Œï¼‰|
| 2022 | `CVPR` | [Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation](https://arxiv.org/abs/2203.02764) | [![Github stars](https://img.shields.io/github/stars/YicongHong/Discrete-Continuous-VLN.svg)](https://github.com/YicongHong/Discrete-Continuous-VLN) |---|
| 2021 | `CVPR` | [Scene-Intuitive Agent for Remote Embodied Visual Grounding](https://arxiv.org/pdf/2103.12944) |---|---|
| 2021 | `NeurIPS` | [SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2110.14143) |---|---|
| 2021 | `ICCV` | [The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YuankaiQi/ORIST.svg)](https://github.com/YuankaiQi/ORIST) |---|
| 2021 | `CVPR` | [VLN BERT: A Recurrent Vision-and-Language BERT for Navigation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YicongHong/Recurrent-VLN-BERT.svg)](https://github.com/YicongHong/Recurrent-VLN-BERT) |---|
| 2021 | EACL | [On the Evaluation of Vision-and-Language Navigation Instructions](https://arxiv.org/abs/2101.10504) |---|---|
|---|---| [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://say-can.github.io/assets/palm_saycan.pdf) | [![Github stars](https://img.shields.io/github/stars/say-can/say-can.github.io.svg)](https://say-can.github.io/) |---|
|2021|`NIPS`<br>PSL Research University|[History aware multimodal transformer for vision-and-language navigation](https://proceedings.neurips.cc/paper/2021/file/2e5c2cb8d13e8fba78d95211440ba326-Paper.pdf)|[![Github stars](https://img.shields.io/github/stars/cshizhe/VLN-HAMT.svg)](https://github.com/cshizhe/VLN-HAMT)<br>[website](https://cshizhe.github.io/projects/vln_hamt.html)|æå‡ºHistory Aware Multimodal Transformer (HAMT)ï¼Œé€šè¿‡transformerå°†é•¿æœŸå†å²çº³å…¥VLNå†³ç­–ä¸­;å…ˆé‡‡ç”¨ViTæ¥ç¼–ç æ¯å¼ ç‹¬ç«‹çš„å›¾åƒï¼Œç„¶åå°†å…¨æ™¯è§‚æµ‹ä¸æ¯å¼ å›¾åƒçš„ç©ºé—´å…³ç³»å»ºæ¨¡ï¼Œæœ€åæŠŠä¸åŒå…¨æ™¯å›¾çš„æ—¶é—´ç»´åº¦çš„å…³è”è€ƒè™‘åœ¨å†…|
|2021|`CVPR`|[Room-and-object aware knowledge reasoning for remote embodied referring expression](https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Room-and-Object_Aware_Knowledge_Reasoning_for_Remote_Embodied_Referring_Expression_CVPR_2021_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/alloldman/CKR.svg)](https://github.com/alloldman/CKR)|---|
|2021|`ICCV`<br>Chinese Academy of Sciences|[Vision-language navigation with random environmental mixup](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Vision-Language_Navigation_With_Random_Environmental_Mixup_ICCV_2021_paper.pdf)|---|é€šè¿‡æ··åˆæŒ‡ä»¤ä¸å¯¹åº”çš„è½¨è¿¹/ç¯å¢ƒï¼Œå®ç°VLNæ•°æ®å¢å¹¿|
|2021|`ICRA`<br>Georgia Institute of Technology |[Hierarchical cross-modal agent for robotics vision-and-language navigation](https://arxiv.org/pdf/2104.10674)|---|Robo-VLN <br> é¦–ç¯‡é’ˆå¯¹è¿ç»­3Dç©ºé—´çš„VLNä»»åŠ¡;é‡‡ç”¨çš„ç±»ä¼¼åŒç³»ç»Ÿæ¶æ„ï¼šhigh-level policyï¼šå°†è¾“å…¥çš„æ–‡æœ¬åŠè§†è§‰æ¨¡æ€è¿›è¡Œå¯¹é½åŠæ¨ç†ï¼Œå¹¶è¾“å‡ºsub-goalï¼›low-level policyï¼šå°†æ¥è‡ªé«˜å±‚ç­–ç•¥çš„sub-goal outputè½¬æ¢ä¸ºåº•å±‚actionï¼Œå¹¶ä¸”é€šè¿‡æ¨¡ä»¿å­¦ä¹ ç­–ç•¥æœ‰æ•ˆåœ°æ¨¡ä»¿ä¸“å®¶æ§åˆ¶å™¨ï¼›ç½‘ç»œæ¶æ„æ–¹é¢ï¼šè¯­è¨€æŒ‡ä»¤ç”±BERT embeddingï¼ŒRGBDæ•°æ®ç”±CNNæå–ç‰¹å¾ï¼Œé€šè¿‡Transformerå°†è§†è§‰ä¸æ–‡æœ¬ç‰¹å¾èåˆï¼Œå¯¹äºdecoderé‡‡ç”¨RNNï¼Œå¯¹äºlow-level actionç›´æ¥è¾“å‡ºagentçš„çº¿é€Ÿåº¦ä¸è§’é€Ÿåº¦| 
| 2020 | `CVPR`<br>Duke University | [Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training](https://arxiv.org/abs/2002.10638) | [![Github stars](https://img.shields.io/github/stars/weituo12321/PREVALENT.svg)](https://github.com/weituo12321/PREVALENT) |é¦–æ¬¡æå‡ºé€šè¿‡é¢„è®­ç»ƒä»¥åŠfine-tuningçš„èŒƒå¼æ¥å®ç°VLNä»»åŠ¡|
|2020|`ECCV`<br>Beijing Institute of Technology|[Active visual information gathering for vision-language navigation](https://arxiv.org/pdf/2007.08037)|[![Github stars](https://img.shields.io/github/stars/HanqingWangAI/Active_VLN.svg)](https://github.com/HanqingWangAI/Active_VLN)|å½“æŒ‡ä»¤æ¨¡ç³Šæˆ–è§‚æµ‹ä¸å¤Ÿå¯¼è‡´ä¸ç¡®å®šæ—¶ï¼Œæ‰§è¡Œactive explorationï¼›å…³é”®åœ¨äºï¼šå†³å®šä»€ä¹ˆæ—¶å€™å±•å¼€æ¢ç´¢ã€å“ªä¸€éƒ¨åˆ†å€¼å¾—æ¢ç´¢|
|2020|`CVPR`<br>|[Vision-language navigation with self-supervised auxiliary reasoning tasks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Vision-Language_Navigation_With_Self-Supervised_Auxiliary_Reasoning_Tasks_CVPR_2020_paper.pdf)|---|ä»è¯­ä¹‰ä¿¡æ¯ä¸­è·å–å››ä¸ªé¢å¤–çš„è®­ç»ƒä¿¡å·ï¼šè¿‡å»çš„actionã€å¯¼èˆªçš„è¿›ç¨‹ã€ä¸‹ä¸€åŠ¨ä½œæ˜¯å¦ä¸æŒ‡ç¤ºç›¸åŒ¹é…ã€è½¨è¿¹çš„ä¸€è‡´æ€§ï¼›ç½‘ç»œæ¶æ„ä¸ºLSTM+Attention+CNN|
|2020|`ECCV`<br>Georgia Institute of Technology|[Improving vision-and-language navigation with image-text pairs from the web](https://arxiv.org/pdf/2004.14973)|---|VLN-BERT<br>é€šè¿‡ç½‘ç»œæŠ“å–çš„è§†è§‰å’Œè¯­è¨€è¯­æ–™åº“æ¥è®©æ¨¡å‹å­¦ä¼švisual groundingsï¼Œè¿›è€Œæå‡æ€§èƒ½ï¼›é‡‡ç”¨äº†ViLBERT+BERT|
| 2020 | `ECCV` | [Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/abs/2004.02857) | [![Github stars](https://img.shields.io/github/stars/jacobkrantz/VLN-CE.svg)](https://github.com/jacobkrantz/VLN-CE) |---|
|2019|`EMNLP`|[Robust navigation with language pretraining and stochastic sampling](https://arxiv.org/pdf/1909.02244)|[![Github stars](https://img.shields.io/github/stars/xjli/r2r_vln.svg)](https://github.com/xjli/r2r_vln)|---|
|2019|`CoRL`|[Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight](https://arxiv.org/pdf/1910.09664)|[![Github stars](https://img.shields.io/github/stars/lil-lab/drif.svg)](https://github.com/lil-lab/drif)|---|
|2018|`NIPS`<br> University of California|[Speaker-follower models for vision-and-language navigation](https://arxiv.org/pdf/1806.02724)|[![Github stars](https://img.shields.io/github/stars/ronghanghu/speaker_follower.svg)](https://github.com/ronghanghu/speaker_follower)<br>[website](https://ronghanghu.com/speaker_follower/)|å°†VLNä»»åŠ¡è§†ä¸ºè½¨è¿¹æœç´¢é—®é¢˜:æŒ‡ä»¤è§£ææ¨¡å—(followerå°†æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œç©ºé—´)+æŒ‡ä»¤ç”Ÿæˆæ¨¡å—(speakerå°†åŠ¨ä½œåºåˆ—æ˜ å°„å›æŒ‡ä»¤),ä¸¤è€…å‡ä¸ºseq2seqæ¶æ„ï¼›speaker modelå¯ä»¥é€šè¿‡çœŸå€¼å¯¼èˆªè·¯çº¿ä¸æŒ‡ä»¤è¿›è¡Œè®­ç»ƒï¼›åœ¨followeræµ‹è¯•çš„æ—¶å€™ï¼Œfollowerä¼šç”Ÿæˆç»™å®šçš„æŒ‡ä»¤çš„æ½œåœ¨è·¯çº¿ã€‚è€Œspeakerå¯¹è¿™äº›è·¯çº¿è¿›è¡Œæ’åï¼Œé€‰æ‹©èƒ½å¤Ÿæ›´å¥½çš„è§£ææŒ‡ä»¤çš„ä¸€æ¡ï¼ˆç±»ä¼¼GANç½‘ç»œçš„æ€è·¯ï¼‰|
|2018|`RSS`|[Following high-level navigation instructions on a simulated quadcopter with imitation learning](https://arxiv.org/pdf/1806.00047)|[![Github stars](https://img.shields.io/github/stars/lil-lab/gsmn.svg)](https://github.com/lil-lab/gsmn)|---|
|2018|`CVPR`<br>Australian National University|[Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments](https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/peteanderson80/Matterport3DSimulator.svg)](https://github.com/peteanderson80/Matterport3DSimulator)<br>[website](https://bringmeaspoon.org/)|ç¬¬ä¸€ç¯‡æå‡ºVLNä»»åŠ¡çš„è®ºæ–‡(ç¦»æ•£)ï¼›æå‡ºR2Ræ•°æ®é›†ï¼ˆMatterport3Dï¼Œ2Wå¤šæ¡å¯¼èˆªæŒ‡ä»¤ï¼‰ï¼›ç½‘ç»œç»“æ„éƒ¨åˆ†ï¼šè¯­è¨€é‡‡ç”¨çš„æ˜¯åŸºäºLSTMçš„seqtoseq ç»“æ„å’Œæ³¨æ„æœºåˆ¶ï¼Œå›¾åƒåˆ™æ˜¯ç”¨çš„ImageNetä¸Šé¢„è®­ç»ƒçš„ResNet-152 CNNï¼›|

<br><br><br>

## Simulator and Dataset

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/pdf/2511.17889)|[![Github stars](https://img.shields.io/github/stars/AIGeeksGroup/MobileVLA-R1.svg)](https://github.com/AIGeeksGroup/MobileVLA-R1)|MobileVLA-CoTæ•°æ®é›†<br>[website](https://aigeeksgroup.github.io/MobileVLA-R1/)|
|2025|`arXiv`|[VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.17221)| [![Github stars](https://img.shields.io/github/stars/Qi-Zhangyang/GPT4Scene-and-VLN-R1.svg)](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1)|VLN-Ego datasetï¼ˆHabitat simulatorï¼‰<br>[website](https://vlnr1.github.io/)|
|2025|`arXiv`|[InternVLA-N1: An Open Dual-System Vision-Language Navigation Foundation Model with Learned Latent Plans](https://internrobotics.github.io/internvla-n1.github.io/static/pdfs/InternVLA_N1.pdf)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternNav.svg)](https://github.com/InternRobotics/InternNav) |[website](https://internrobotics.github.io/internvla-n1.github.io/)<br>InternData-N1 Dataset|
|2025|`arXiv`|[HA-VLN: A Benchmark for Human-Aware Navigation in Discreteâ€“Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard](https://arxiv.org/pdf/2503.14229)|[![Github stars](https://img.shields.io/github/stars/F1y1113/HA-VLN.svg)](https://github.com/F1y1113/HA-VLN)|[websit](https://ha-vln-project.vercel.app/)|
|2023|`ICCV`|[Learning vision-and-language navigation from youtube videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Learning_Vision-and-Language_Navigation_from_YouTube_Videos_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/JeremyLinky/YouTube-VLN.svg)](https://github.com/JeremyLinky/YouTube-VLN)|YouTube-VLN|
|2023|`ICCV`|[Aerialvln: Vision-and-language navigation for uavs](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_AerialVLN_Vision-and-Language_Navigation_for_UAVs_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/AirVLN/AirVLN.svg)](https://github.com/AirVLN/AirVLN)|AerialVLN|
|2023|`ICCV`|[Scaling data generation in vision-and-language navigation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/wz0919/ScaleVLN.svg)](https://github.com/wz0919/ScaleVLN)|ScaleVLN|
|2022|`CVPR`|[Habitat-web: Learning embodied object-search strategies from human demonstrations at scale](https://openaccess.thecvf.com/content/CVPR2022/papers/Ramrakhya_Habitat-Web_Learning_Embodied_Object-Search_Strategies_From_Human_Demonstrations_at_Scale_CVPR_2022_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/Ram81/habitat-web.svg)](https://github.com/Ram81/habitat-web)|[website](https://ram81.github.io/projects/habitat-web)|
|2022|`CVPR`|[Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation](https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Bridging_the_Gap_Between_Learning_in_Discrete_and_Continuous_Environments_CVPR_2022_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/YicongHong/Discrete-Continuous-VLN.svg)](https://github.com/YicongHong/Discrete-Continuous-VLN)|R2R-CE|
|2021|`CVPR`|[Soon: Scenario oriented object navigation with graph-based exploration](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_SOON_Scenario_Oriented_Object_Navigation_With_Graph-Based_Exploration_CVPR_2021_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/ZhuFengdaaa/SOON.svg)](https://github.com/ZhuFengdaaa/SOON)|SOON|
|2020|`arXiv`|[Alfred: A benchmark for interpreting grounded instructions for everyday tasks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Shridhar_ALFRED_A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_CVPR_2020_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/askforalfred/alfred.svg)](https://github.com/askforalfred/alfred) |ALFRED<br>[website](AskForALFRED.com)| 
|2020|`CVPR`|[REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments](https://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_REVERIE_Remote_Embodied_Visual_Referring_Expression_in_Real_Indoor_Environments_CVPR_2020_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/YuankaiQi/REVERIE.svg)](https://github.com/YuankaiQi/REVERIE)|REVERIE|
|2020|`EMNLP`|[Where are you? localization from embodied dialog](https://arxiv.org/pdf/2011.08277)|---|[website](https://meerahahn.github.io/way/)|
|2020|`CoRL`|[Vision-and-Dialog Navigation](https://arxiv.org/pdf/1907.04957)|[![Github stars](https://img.shields.io/github/stars/mmurray/cvdn.svg)](https://github.com/mmurray/cvdn/)|CVDN<br>[website](https://cvdn.dev/)|
|2020|`EMNLP`|[Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding](https://arxiv.org/pdf/2010.07954)|[![Github stars](https://img.shields.io/github/stars/google-research-datasets/RxR.svg)](https://github.com/google-research-datasets/RxR)|RxR|
|2019|`EMNLP`|[Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning](https://arxiv.org/pdf/1909.01871)|[![Github stars](https://img.shields.io/github/stars/khanhptnk/hanna.svg)](https://github.com/khanhptnk/hanna)|HANNA|
|2019|`ACL`|[Stay on the path: Instruction fidelity in vision-and-language navigation](https://arxiv.org/pdf/1905.12255)|---|R4R<br>[website](https://github.com/google-research/google-research/tree/master/r4r)|
|2019|`arXiv`|[Learning to navigate unseen environments: Back translation with environmental dropout](https://arxiv.org/pdf/1904.04195)|[![Github stars](https://img.shields.io/github/stars/airsplay/R2R-EnvDrop.svg)](https://github.com/airsplay/R2R-EnvDrop)|R2R-EnvDrop-CE|
|2018|`CVPR`|[Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments](https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/peteanderson80/Matterport3DSimulator.svg)](https://github.com/peteanderson80/Matterport3DSimulator)|R2R<br>[website](https://bringmeaspoon.org/)|


## Survey Paper
<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey](https://arxiv.org/pdf/2508.15354)|[![Github stars](https://img.shields.io/github/stars/Franky-X/Awesome-Embodied-Navigation.svg)](https://github.com/Franky-X/Awesome-Embodied-Navigation)|Survey for EN <br> [blog](https://kwanwaipang.github.io/Enbodied-Navigation/)|
|2025|`Transactions on Mechatronics`|[Aligning cyber space with physical world: A comprehensive survey on embodied ai](https://arxiv.org/pdf/2407.06886?)|[![Github stars](https://img.shields.io/github/stars/HCPLab-SYSU/Embodied_AI_Paper_List.svg)](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)|---|
|2024|`Transactions on Machine Learning Research`|[Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models](https://openreview.net/pdf?id=yiqeh2ZYUh)|[![Github stars](https://img.shields.io/github/stars/zhangyuejoslin/VLN-Survey-with-Foundation-Models.svg)](https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models)|[blog](https://kwanwaipang.github.io/VLNsurvery2024/)|
|2024|`arXiv`|[A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/pdf/2405.14093)|[![Github stars](https://img.shields.io/github/stars/yueen-ma/Awesome-VLA.svg)](https://github.com/yueen-ma/Awesome-VLA)|Survey for VLA|
|2024|`Neural Computing and Applications`|[Vision-language navigation: a survey and taxonomy](https://arxiv.org/pdf/2108.11544)|---|---|
|2023|`Artificial Intelligence Review`|[Visual language navigation: A survey and open challenges](https://link.springer.com/article/10.1007/s10462-022-10174-9)|---|---|
|2022|`ACL`|[Vision-and-language navigation: A survey of tasks, methods, and future directions](https://arxiv.org/pdf/2203.12667)|[![Github stars](https://img.shields.io/github/stars/eric-ai-lab/awesome-vision-language-navigation.svg)](https://github.com/eric-ai-lab/awesome-vision-language-navigation)|---|



<!-- 

<br>
| Simulator | Dataset | Link | Note |
|:---------:|:-------:| ---- |:----:|
|VizDooma|---|[website](https://vizdoom.cs.put.edu.pl/)|---|
|House3D|SUNCG|[website](https://github.com/facebookresearch/House3D)|---|
|AI2THOR|---|[website](http://ai2thor.allenai.org)|---|
|Gibson|2D-3D-S|[website](http://gibsonenv.stanford.edu/)|---|
|iGibson|iGibson|[website](http://gibsonenv.stanford.edu/)|---|
|Matterport3DSimulator|R2R, R4R, REVERIE, SOON|---|
|Habitat|VLN-CE|[website](https://aihabitat.org/)|---|
|AirSim|AerialVLN|[website](https://github.com/microsoft/AirSim)|---|

-->




# Learning-based Navigation
or Image-goal Navigation, or object-goal navigation
<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`TRO`|[Neupan: Direct point robot navigation with end-to-end model-based learning](https://arxiv.org/pdf/2403.06828)| [![Github stars](https://img.shields.io/github/stars/hanruihua/NeuPAN.svg)](https://github.com/hanruihua/NeuPAN)<br> [![Github stars](https://img.shields.io/github/stars/hanruihua/neupan_ros.svg)](https://github.com/hanruihua/neupan_ros)<br> [![Github stars](https://img.shields.io/github/stars/KevinLADLee/neupan_ros2.svg)](https://github.com/KevinLADLee/neupan_ros2) |[website](https://hanruihua.github.io/neupan_project/)|
|2025|`arXiv`|[Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation](https://arxiv.org/pdf/2510.08713)|[![Github stars](https://img.shields.io/github/stars/F1y1113/UniWM.svg)](https://github.com/F1y1113/UniWM)|---|
|2025|`arXiv`|[NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance](https://arxiv.org/pdf/2505.08712)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/NavDP.svg)](https://github.com/InternRobotics/NavDP)|[website](https://wzcai99.github.io/navigation-diffusion-policy.github.io/)|
|2025|`arXiv`|[MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation](https://arxiv.org/pdf/2511.10376v2)|[![Github stars](https://img.shields.io/github/stars/ylwhxht/MSGNav.svg)](https://github.com/ylwhxht/MSGNav)|---|
|2025|`arXiv`|[Adaptive Interactive Navigation of Quadruped Robots using Large Language Models](https://arxiv.org/pdf/2503.22942?)|---|[Video](https://www.youtube.com/watch?v=W5ttPnSap2g)|
|2025|`arXiv`|[DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction](https://www.arxiv.org/pdf/2510.07152)|---|---|
|2025|`arXiv`|[IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/pdf/2508.00823)|[![Github stars](https://img.shields.io/github/stars/GWxuan/IGL-Nav.svg)](https://github.com/GWxuan/IGL-Nav)|[website](https://gwxuan.github.io/IGL-Nav/)<br>Exploration+target matching|
|2025|`arXiv`|[LOVON: Legged Open-Vocabulary Object Navigator](https://arxiv.org/pdf/2507.06747?)|[![Github stars](https://img.shields.io/github/stars/DaojiePENG/LOVON.svg)](https://github.com/DaojiePENG/LOVON)|[website](https://daojiepeng.github.io/LOVON/)|
|2025|`ICRA`|[TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals](https://arxiv.org/pdf/2509.08699)|[![Github stars](https://img.shields.io/github/stars/podgorki/TANGO.svg)](https://github.com/podgorki/TANGO)|[website](https://podgorki.github.io/TANGO/)|
|2025|`RSS`|[Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation](https://arxiv.org/pdf/2504.19322)|[![Github stars](https://img.shields.io/github/stars/leggedrobotics/fdm.svg)](https://github.com/leggedrobotics/fdm)|[website](https://leggedrobotics.github.io/fdm.github.io/)|
|2025|`arXiv`|[Parkour in the Wild: Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Distillation and RL Fine-tuning](https://arxiv.org/pdf/2505.11164)|---|---|
|2025|`CoRL`|[Omni-Perception: Omnidirectional Collision Avoidance for Legged Locomotion in Dynamic Environments](https://arxiv.org/pdf/2505.19214)|[![Github stars](https://img.shields.io/github/stars/aCodeDog/OmniPerception.svg)](https://github.com/aCodeDog/OmniPerception)|---|
|2024|`ICRA`|[VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation](https://arxiv.org/pdf/2312.03275)|[![Github stars](https://img.shields.io/github/stars/bdaiinstitute/vlfm.svg)](https://github.com/bdaiinstitute/vlfm)|[website](https://naoki.io/portfolio/vlfm)|
|2024|`SRO`|[Learning Robust Autonomous Navigation and Locomotion for Wheeled-Legged Robots](https://arxiv.org/pdf/2405.01792)|---|---| 
|2024|`RAL`|[PIE: Parkour With Implicit-Explicit Learning Framework for Legged Robots](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678805)|---|---| 
|2024|`ICRA`|[Extreme Parkour with Legged Robots](https://arxiv.org/pdf/2309.14341)|[![Github stars](https://img.shields.io/github/stars/chengxuxin/extreme-parkour.svg)](https://github.com/chengxuxin/extreme-parkour)|[website](https://extreme-parkour.github.io/)| 
|2023|`ICML`|[Esc: Exploration with soft commonsense constraints for zero-shot object navigation](https://proceedings.mlr.press/v202/zhou23r/zhou23r.pdf)|---|---|
|2023|`ICRA`|[Zero-shot object goal visual navigation](https://arxiv.org/pdf/2206.07423)|[![Github stars](https://img.shields.io/github/stars/pioneer-innovation/Zero-Shot-Object-Navigation.svg)](https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation)|---|
|2023|`ICRA`|[ViNL: Visual Navigation and Locomotion Over Obstacles](https://arxiv.org/pdf/2210.14791)|[![Github stars](https://img.shields.io/github/stars/SimarKareer/ViNL.svg)](https://github.com/SimarKareer/ViNL)|[website](https://www.joannetruong.com/projects/vinl.html)|
|2023|`Field Robotics`|[ArtPlanner: Robust Legged Robot Navigation in the Field](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10876046)|[![Github stars](https://img.shields.io/github/stars/leggedrobotics/art_planner.svg)](https://github.com/leggedrobotics/art_planner)|---|


## Mapless navigation
<!-- |---|`arXiv`<br>|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:| ----------- |
|2025|`arXiv`<br>TUM|[Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?](https://arxiv.org/pdf/2511.17792)|[![Github stars](https://img.shields.io/github/stars/TUM-AVS/target-bench.svg)](https://github.com/TUM-AVS/target-bench)<br> [website](https://target-bench.github.io/)<br> [Dataset](https://huggingface.co/target-bench)|æ•°æ®é›†Target-Bench: 450 ä¸ªè§†é¢‘ï¼ˆ112,500 å¸§ï¼‰ï¼Œè¦†ç›– 45 ç§è¯­ä¹‰ç›®æ ‡ç±»åˆ«ï¼Œæ¶‰åŠå¤šç§å®¤å†…å¤–ç¯å¢ƒã€‚æ•°æ®é›†åŒ…å«SLAMè½¨è¿¹ã€äººç±»æ ‡æ³¨çš„æ˜¾å¼å’Œéšå¼ç›®æ ‡ï¼Œæ•°æ®é›†å¹³å°ä¸ºå››è¶³æœºå™¨äººï¼ˆAGX Orin+Livox+åŒç›®ï¼‰; é¦–æ¬¡æå‡ºä¸–ç•Œæ¨¡å‹åœ¨æ— å›¾è·¯å¾„è§„åˆ’ä¸­çš„è¯„ä»·æ¡†æ¶ï¼ˆè·¯å¾„è¯„ä¼°æ¨¡å—ï¼‰ï¼›ä¸–ç•Œè§£ç å™¨(world decoder)ï¼šæ—¶ç©ºé‡å»ºæ¢å¤è½¨è¿¹ï¼ˆVGGT/SpaTracker/ViPEï¼‰+å°ºåº¦æ¢å¤ï¼ˆViPEä¸ºè§†è§‰æƒ¯æ€§SLAMä¸éœ€è¦ï¼Œå…¶ä»–ä¸¤è€…éœ€è¦çœŸå€¼å¯¹é½ï¼‰;è¯„ä¼°çš„ä¸–ç•Œæ¨¡å‹åŒ…æ‹¬äº†Soraã€Veoã€Wanä¸‰ä¸ªç³»åˆ—åŠå…¶å˜ä½“|
|2025|`RSS`|[CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance](https://arxiv.org/pdf/2503.03921?)|[![Github stars](https://img.shields.io/github/stars/ut-amrl/creste_public.svg)](https://github.com/ut-amrl/creste_public)<br> [website](https://amrl.cs.utexas.edu/creste/)|---|


# Others
<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`IEEE/ASME Transactions on Mechatronics`|[Aligning cyber space with physical world: A comprehensive survey on embodied ai](https://arxiv.org/pdf/2407.06886)|[![Github stars](https://img.shields.io/github/stars/HCPLab-SYSU/Embodied_AI_Paper_List.svg)](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)|---|
|2025|`arXiv`|[HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots](https://arxiv.org/pdf/2503.09010)|---|---|
|2021|`ICML`|[Learning transferable visual models from natural language supervision](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf)|[![Github stars](https://img.shields.io/github/stars/OpenAI/CLIP.svg)](https://github.com/OpenAI/CLIP)|CLIP<br>[website](https://openai.com/index/clip/)|

## Occupancy Perception
<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots](https://arxiv.org/pdf/2507.20217)|[![Github stars](https://img.shields.io/github/stars/Open-X-Humanoid/Humanoid-Occupancy.svg)](https://github.com/Open-X-Humanoid/Humanoid-Occupancy)|[website](https://humanoid-occupancy.github.io/)<br>Multimodal Occupancy Perception| 
|2025|`arXiv`|[Roboocc: Enhancing the geometric and semantic scene understanding for robots](https://arxiv.org/pdf/2504.14604)|---|3DGS|
|2025|`ICCV`|[EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding](https://arxiv.org/pdf/2412.04380)|[![Github stars](https://img.shields.io/github/stars/YkiWu/EmbodiedOcc.svg)](https://github.com/YkiWu/EmbodiedOcc)|[website](https://ykiwu.github.io/EmbodiedOcc/)|
|2023|`ICCV`|[Scene as occupancy](https://openaccess.thecvf.com/content/ICCV2023/papers/Tong_Scene_as_Occupancy_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/OpenDriveLab/OccNet.svg)](https://github.com/OpenDriveLab/OccNet)|[Challenge and dataset](https://github.com/OpenDriveLab/OpenScene)|




