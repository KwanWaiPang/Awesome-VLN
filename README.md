<p align="center">
  <h1 align="center">
  Awesome Visual-Language-Navigation (VLN)
  </h1>
</p>

This repository contains a curated list of resources addressing the VLN (Visual Language Navigation).
Additionally, it includes related papers from areas such as learning-based end-to-end navigation and mapless navigation.

If you find some ignored papers, **feel free to [*create pull requests*](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM/blob/pdf/How-to-PR.md), or [*open issues*](https://github.com/KwanWaiPang/Awesome-VLN/issues/new)**. 

Contributions in any form to make this list more comprehensive are welcome.

If you find this repository useful, a simple star should be the best affirmation. ğŸ˜Š

Feel free to share this list with others!

* Paper Survey for [Visual-Language-Action (VLA)](https://github.com/KwanWaiPang/Awesome-VLA)

# Overview
- [VLN](#VLN)
  - [Simulator and Dataset](#Simulator-and-Dataset)
  - [Survey Paper](#Survey-Paper)
- [Learning-based Navigation](#Learning-based-Navigation)
  - [Mapless navigation](#Mapless-navigation) 
- [Others](#Others)
  - [Occupancy Perception](#Occupancy-Perception)

# VLN
<!-- |---|`arXiv`<br>|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|-----------|
|2026|`arXiv`<br>ByteDance Seed|[Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning](https://arxiv.org/pdf/2602.09972)|[website](https://zixuan-wang99.github.io/Hydra-Nav/)|å•ä¸€VLMæ¡†æ¶ï¼ˆQwen2.5-VL-7Bï¼‰å®ç°åœ¨å¿«æ…¢åŒç³»ç»Ÿï¼ˆå¿«ç³»ç»Ÿï¼šé«˜æ•ˆä½çº§åˆ«æ‰§è¡Œï¼›æ…¢ç³»ç»Ÿï¼šå…¨å±€è§„åˆ’ä¸æ—¶ç©ºæ¨ç†ï¼‰é—´è‡ªé€‚åº”åˆ‡æ¢ï¼›çœŸæœºéƒ¨ç½²äºUnitree Go2å››è¶³æœºå™¨äººï¼Œé€šè¿‡éçº¿æ€§æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆNMPCï¼‰æ‰§è¡ŒåŠ¨ä½œï¼ˆæ¨ç†åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼‰|
|2026|`arXiv`<br>AMAP, Alibaba Group|[ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation](https://arxiv.org/pdf/2602.11598)|[![Github stars](https://img.shields.io/github/stars/amap-cvlab/ABot-Navigation.svg)](https://github.com/amap-cvlab/ABot-Navigation) <br>[website](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)|ç»Ÿä¸€æ¨¡å‹å®ç°ç‚¹ç›®æ ‡ã€ç‰©ä½“ç›®æ ‡ã€æŒ‡ä»¤è·Ÿéšã€å…´è¶£ç‚¹å¯¼èˆªã€è¡Œäººè·Ÿéšï¼›LLMï¼ˆQwen3-4Bï¼‰è´Ÿè´£è¯­ä¹‰æ¨ç†ï¼Œflow matchingé©±åŠ¨action expertç”Ÿæˆè½¨è¿¹ï¼Œ7802 ä¸ª 3D åœºæ™¯ï¼ˆæ€»é¢ç§¯é«˜è¾¾10.7å¹³æ–¹å…¬é‡Œï¼‰ã€1690 ä¸‡æ¡è½¨è¿¹ä¸ 500 ä¸‡æ¡æ¨ç†æ ·æœ¬çš„å¤§è§„æ¨¡æ•°æ®å¼•æ“ï¼›éƒ¨ç½²äºNVIDIA Jetson Orin NXï¼Œ2HZ|
|2026|`arXiv`<br>Zhejiang University|[NavDreamer: Video Models as Zero-Shot 3D Navigators](https://arxiv.org/pdf/2602.09765)|[website](https://xinjiu612.github.io/NavDreamer/)|è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼ˆæ¨ç†è½¨è¿¹ï¼‰+Qwen3-VLï¼ˆè½¨è¿¹é€‰æ‹©ï¼‰+Ego-Planneræ¥æ”¶3Dä½ç½®ç›®æ ‡+ä¼ ç»Ÿè¿æ§+Fast-LIVO2å®šä½|
|2026|`arXiv`<br>Amap, Alibaba Group|[MerNav: A Highly Generalizable Memoryâ€“Executeâ€“Review Framework for Zero-Shot Object Goal Navigation](https://arxiv.org/pdf/2602.05467)|---|Object Goal Navigation,æ„å»ºâ€œè®°å¿†-æ‰§è¡Œ-å¤ç›˜â€ï¼ˆMemory-Execute-Reviewï¼‰æ¡†æ¶:é€šè¿‡åˆ†å±‚è®°å¿†æ¨¡å—ï¼ˆâ€œçŸ­-é•¿-å¸¸è¯†â€ä¸‰å±‚è®°å¿†ç³»ç»Ÿï¼‰ç§¯ç´¯ä¿¡æ¯ï¼Œæ‰§è¡Œæ¨¡å—ï¼ˆè§‚æµ‹åˆ†æã€è·¯å¾„è§„åˆ’ã€åŠ¨ä½œé€‰æ‹©ã€åœæ­¢å†³ç­–ï¼‰åº”å¯¹å¸¸è§„å¯¼èˆªï¼Œå¤ç›˜æ¨¡å—ï¼ˆé¿éšœè„±å›°ã€è·¨åŒºåŸŸæ¢ç´¢ï¼‰ä¿®æ­£å¼‚å¸¸è¡Œä¸ºã€‚|
|2026|`arXiv`<br> Nanjing University of Aeronautics and Astronautics|[VPN: Visual Prompt Navigation](https://arxiv.org/pdf/2508.01766v6)|[![Github stars](https://img.shields.io/github/stars/farlit/VPN.svg)](https://github.com/farlit/VPN)|è§†è§‰æç¤ºå¯¼èˆª:ä»…ä¾é ç”¨æˆ·åœ¨2Dé¡¶è§†å›¾åœ°å›¾ä¸­æä¾›çš„è§†è§‰æç¤ºå¼•å¯¼æ™ºèƒ½ä½“å¯¼èˆªï¼Œæ—¢é™ä½äº†éä¸“ä¸šç”¨æˆ·çš„ä½¿ç”¨é—¨æ§›ï¼Œåˆå‡å°‘äº†æŒ‡ä»¤çš„æ­§ä¹‰æ€§ã€‚æå‡ºbaselineï¼šVPNetï¼Œä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ï¼šR2R-VPå’ŒR2R-CE-VP|
|2026|`arXiv`<br>Adelaide University|[SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/pdf/2601.06806)|---|é›¶æ ·æœ¬VLNï¼Œæ™ºèƒ½ä½“åœ¨æ‰§è¡Œå¯¼èˆªä»»åŠ¡å‰å¯¹ç¯å¢ƒè¿›è¡Œå……åˆ†é¢„æ¢ç´¢ï¼ˆä¹Ÿå°±æ˜¯SLAMï¼‰ï¼›æ„å»ºäº†ç©ºé—´åœºæ™¯å›¾ï¼ˆSSGï¼‰ï¼Œé€šè¿‡å¯¹RGB-SLAMè·å–çš„3Dç‚¹äº‘æ·»åŠ ç»“æ„åŒ–ç©ºé—´æ ‡æ³¨ï¼ˆå¦‚æ¥¼å±‚-æˆ¿é—´-ç‰©ä½“çš„å±‚çº§å…³ç³»ï¼‰å’Œè¯­ä¹‰æ ‡ç­¾ï¼ˆå¦‚æˆ¿é—´ç±»å‹ã€ç‰©ä½“ç±»åˆ«ï¼‰ï¼›SpatialNavæ™ºèƒ½ä½“åŸºäºMLLMï¼ˆGPT-5.1ï¼‰ï¼›R2Rçš„SRä¸º59.3%ï¼ŒREVERIEçš„SRä¸º50.4%ï¼ŒR2R-CEä¸º68.0%ï¼ŒRxR-CEä¸º39.0%|
|2026|`arXiv`<br>North University of China|[SeqWalker: Sequential-Horizon Vision-and-Language Navigation with Hierarchical Planning](https://arxiv.org/pdf/2601.04699v1)|[![Github stars](https://img.shields.io/github/stars/SeqWalker/SeqWalker-code.svg)](https://github.com/SeqWalker/SeqWalker-code)<br>[website](https://seqwalker.github.io/seqwalker/)|æå‡ºåºåˆ—è§†é‡è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆSH-VLNï¼‰ä»»åŠ¡ï¼Œè¦æ±‚æ™ºèƒ½ä½“éµå¾ªé•¿åºåˆ—è¯­è¨€æŒ‡ä»¤å®Œæˆå¤šä»»åŠ¡è¿ç»­å¯¼èˆªï¼›SeqWalkeræ¨¡å‹ï¼šé«˜å±‚æ„ŸçŸ¥è§„åˆ’ï¼ˆæŒ‡ä»¤åˆ†å‰²æ¨¡å—ï¼ˆISMï¼‰ï¼šCLIP+Qwen-0.5bï¼Œå¯¹é•¿æŒ‡ä»¤è¿›è¡Œå±€éƒ¨åˆ†å‰²ã€åˆ†æ­¥ç†è§£ï¼‰ã€å¯¼èˆªåœºæ™¯æ˜ å°„ï¼ˆå¹¶æ˜¾å¼å­˜å‚¨åœºæ™¯è¯­ä¹‰åœ°å›¾å’Œæ …æ ¼åœ°å›¾ï¼‰ã€åº•å±‚è¿åŠ¨è§„åˆ’ï¼ˆé€šè¿‡æ¢ç´¢-éªŒè¯ï¼ˆEaVï¼‰ç­–ç•¥ï¼Œç”Ÿæˆå…·ä½“å¯¼èˆªåŠ¨ä½œå¹¶ä¿®æ­£è½¨è¿¹è¯¯å·®ï¼‰ï¼›æ‰©å±• IVLN çš„ IR2R-CE æ•°æ®é›†ï¼Œæ„å»º SH IR2R-CE æ•°æ®é›†ï¼šåºåˆ—è½¨è¿¹æ‹¼æ¥+LLaVA-OneVision å……å®é•¿æŒ‡ä»¤|
|2026|`Visual Intelligence`<br>The University of Hong Kong|[NavGemini: a multi-modal LLM agent for vision-and-language navigation](https://link.springer.com/content/pdf/10.1007/s44267-025-00105-x.pdf)|---|å°†å¤šæ¨¡æ€LLMï¼ˆGemini-Pro-Visionï¼‰èå…¥VLNï¼ŒR2Rä¸ŠæˆåŠŸç‡ä¸º3.7%ï¼ŒREVERIEä¸º22.0ï¼›æ²¡æœ‰çœŸæœºå®éªŒ|
|2026|`arXiv`<br>Georgia Institute of Technology|[FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning](https://arxiv.org/pdf/2509.16445v1)|[website](https://filmnav.netlify.app/)|å¾®è°ƒVLMï¼ˆSigLIP ViT+Mamba LLM/2.8Bï¼‰ä½œä¸ºå¯¼èˆªpolicyï¼›å°†è½¨è¿¹å†å²ã€è¯­è¨€ç›®æ ‡å’Œè¾¹ç•Œå›¾åƒæ ¼å¼åŒ–ä¸ºè¾“å…¥åºåˆ—ï¼ŒVLM é¢„æµ‹æœ€ä¼˜è¾¹ç•Œæ ‡è®°ï¼›å°†é€‰æ‹©çš„è¾¹ç•Œåæ ‡ä¼ é€’ç»™é¢„è®­ç»ƒçš„ PointNav ç­–ç•¥ï¼Œè®¡ç®—ä¸‹ä¸€æ­¥çš„ç¦»æ•£åŠ¨ä½œï¼›ä½¿ç”¨ OWLViT v2 æ£€æµ‹ç›®æ ‡ç‰©ä½“ï¼Œå¹¶ä½¿ç”¨ MobileSAM åˆ†å‰²ç‰©ä½“ç‚¹äº‘ï¼›åœ¨ HM3D ObjectNav(ç›®æ ‡å¯¼èˆª)ä¸­çš„SRä¸º61.7%| 
|2026|`arXiv`<br>Beihang University|[AirUniNav: Unified Vision-Language Navigation for UAVs in Indoor and Outdoor Scenes](https://www.techrxiv.org/doi/full/10.36227/techrxiv.176834454.46554529)|[website](https://forhumble.github.io/AirUni/)|æ— äººæœºå…·èº«å¯¼èˆªï¼›æ¨¡å‹æ¶æ„åˆ†ä¸ºï¼šæ–‡æœ¬å¤„ç†æ¨¡å—ï¼ˆå°†æŒ‡ä»¤æ–‡æœ¬åˆ†è§£ä¸ºç¦»æ•£çš„Tokenï¼‰ã€è§†é¢‘ç¼–ç ï¼ˆå°†è§†è§‰è§‚æµ‹åˆ†ä¸ºå†å²å’Œå½“å‰ï¼‰ã€connectorã€LLMï¼ˆQwen 2 7Bï¼‰ï¼›åŠ¨ä½œé›†åŒæ ·æ˜¯ç¦»æ•£åŒ–ï¼Œå®¤å†…ï¼ˆåœæ­¢ã€å‰è¿›ã€å·¦è½¬ã€å³è½¬ï¼‰+å®¤å¤–ï¼ˆå®¤å†…çš„+ä¸Šå‡ã€ä¸‹é™ã€å·¦ç§»ã€å³ç§»ï¼‰ï¼›ä»¿çœŸç¯å¢ƒä¸‹VLN-CE R2Rçš„SRä¸º47.2%ï¼ŒRxRä¸º43.7%ï¼Œå®¤å¤–çš„ä¸º6.8`13.9%;çœŸæœºå®éªŒä¸­ï¼ŒA100éƒ¨ç½²æ¡†æ¶ï¼Œé€šè¿‡APIæ¥æ”¶ä»»åŠ¡æŒ‡ä»¤ï¼Œå®¤å†…å¯¼èˆªæ­¥é•¿ä¸º0.5ç±³ï¼Œå®¤å¤–å¯¼èˆªæ­¥é•¿ä¸º2ç±³ï¼Œæ—‹è½¬åŠ¨ä½œè°ƒæ•´15Â°|
|2026|`RAL`<br>Tsinghua University|[CAUSALNAV: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios](https://arxiv.org/pdf/2601.01872)|---|é¦–ä¸ªåŸºäºåœºæ™¯å›¾çš„è¯­ä¹‰å¯¼èˆªï¼›æ„ŸçŸ¥éƒ¨åˆ†ä¸ºå¼€å‘è¯æ±‡æ„ŸçŸ¥æ¨¡å‹ï¼ˆ YOLOWorldæ£€æµ‹2D bounding boxä»¥åŠè¿›è¡Œåˆ†å‰²ï¼ŒByteTrackè¿›è¡Œå¤šç›®æ ‡è·Ÿè¸ªï¼Œé€šè¿‡LiDARå¯¹ç›®æ ‡è¿›è¡Œå®šä½ï¼‰+LiDARå®šä½ï¼ˆFAST-LIO2ï¼‰ï¼›åˆ©ç”¨LLMï¼ˆè¾¹ç«¯ï¼Œä½œè€…éªŒè¯äº†å››ç§ï¼‰æ„å»ºç²—ç²’åº¦å»ºç­‘ç‰©åœ°å›¾å’Œç»†ç²’åº¦ç›®æ ‡ï¼Œå½¢æˆå¯æ£€ç´¢çš„çŸ¥è¯†åº“ï¼ˆEmbodied Graphï¼Œè¿™éƒ¨åˆ†å‚è€ƒè‡ª[OpenGraph](https://arxiv.org/pdf/2403.09412)ï¼‰ï¼›è§„åˆ’æ¨¡å—ç»“åˆç¦»çº¿åœ°å›¾ï¼ˆsense graphï¼‰ä¸å®æ—¶æ„ŸçŸ¥æ•°æ®ï¼ˆåŸºäºLiDARçš„åŠ¨æ€é¿éšœï¼‰ï¼›ä»¿çœŸä¸‹ï¼Œç³»ç»Ÿè¿è¡Œåœ¨ Intel i9-14900K CPU å’Œ RTX 3090 GPU ä¸Šï¼Œå…³é”®æ¨¡å—å®æ—¶è¿è¡Œï¼ŒåŒ…æ‹¬å¼€æ”¾è¯æ±‡å¯¹è±¡è·Ÿè¸ªï¼ˆ30Hzï¼‰ã€æ—¶ç©ºèµ°å»Šè¿‡æ»¤ï¼ˆ20Hzï¼‰ã€å±€éƒ¨åŠ¨æ€å»ºå›¾ä¸è§„åˆ’ï¼ˆ10Hzï¼‰ä»¥åŠ Embodied Graph æ›´æ–°ï¼ˆ1Hzï¼‰ï¼›å®æµ‹æ—¶ç³»ç»Ÿè¿è¡Œä¸º Intel Core i9-13900H CPU å’Œ NVIDIA GeForce RTX 4070 GPU çš„è½®å¼ç§»åŠ¨æœºå™¨äºº|
|2026|`arXiv`<br>Fantasy AIGC Team|[FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation](https://arxiv.org/pdf/2601.13976)|---|å°†æ–‡æœ¬ã€è§†è§‰å’Œå¤šæ¨¡æ€é“¾å¼æ¨ç†ï¼ˆCoTï¼‰æ¨¡å¼æ•´åˆåˆ°ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œå‹ç¼©è§†è§‰CoTï¼ˆåˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹å°†è§†è§‰tokenç¼–ç åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ï¼‰ï¼Œå¯¼èˆªçš„SRä¸º2.44%|
|2026|`arXiv`<br>Adelaide University|[SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/pdf/2601.06806)|---|å…¨å±€åœ°å›¾å¯ä»¥è®©VLNæ›´å¥½çš„ç†è§£ç©ºé—´ï¼Œè¿›è€Œå…·å¤‡é•¿ç¨‹æ¨ç†èƒ½åŠ›ï¼Œé‡‡ç”¨å…¨å±€ç©ºé—´å»ºæ¨¡ï¼ˆSLAM3Dç‚¹äº‘+äººå·¥åˆ†å‰²ï¼Œç¯å¢ƒçš„å±‚çº§ç»“æ„ã€è¯­ä¹‰æ ‡ç­¾ä¸ç©ºé—´å…³ç³»æ˜¾å¼ç¼–ç ï¼‰ï¼ŒåŸºäºå½“å‰agentçš„ä½ç½®ï¼Œæå–7ç±³åŠå¾„èŒƒå›´çš„ç©ºé—´ï¼ˆZè½´ç¡®å®šæ¥¼å±‚ã€XYç¡®å®šæˆ¿é—´ï¼‰ï¼›å°†360å…¨æ™¯å›¾æ‹†æˆ8å¼ å›¾ï¼Œå°†ç”»é¢æ‹¼æˆ3*3ç½‘æ ¼ï¼ˆä¸­é—´ä¸ºæŒ‡å—é’ˆï¼Œæ˜ç¡®æœºå™¨äººæœå‘ï¼‰ï¼›ä¸”èµ°ä¹‹å‰å…ˆæŸ¥è¯¢å…¨å±€ç©ºé—´åœ°å›¾ç¡®è®¤ä¸‹ä¸€æ­¥çš„ä½ç½®å‘¨å›´æœ‰ä»€ä¹ˆç‰©ä½“ï¼›ä»¿çœŸç¯å¢ƒï¼ŒR2Rï¼ˆ59.3%ï¼‰ã€REVERIEï¼ˆ50.4%ï¼‰å’ŒR2R-CEï¼ˆ68.0%ï¼‰ã€RxR-CEï¼ˆ39.0ï¼‰|
|2026|`arXiv`<br>Nanyang Technological University|[CL-CoTNav: Closed-Loop Hierarchical Chain-of-Thought for Zero-Shot Object-Goal Navigation with Vision-Language Models](https://arxiv.org/pdf/2504.09000)|---|åŸºäºVLMçš„é›¶æ ·æœ¬ç›®æ ‡å¯¼èˆªæ¡†æ¶ï¼šç»“æ„åŒ–æ¨ç†ï¼ˆå°†å¯¼èˆªä»»åŠ¡åˆ†è§£ä¸ºå¤šè½®é—®ç­”ï¼‰+é—­ç¯åé¦ˆæœºåˆ¶ï¼ˆå¼•å…¥ç½®ä¿¡åº¦è¯„åˆ†ï¼Œå¯¹æ¯ä¸ªé—®ç­”å¯é æ€§è¿›è¡Œè¯„ä¼°ï¼‰ï¼Œé‡‡ç”¨LoRAå¯¹2Bçš„æ¨¡å‹ï¼ˆInternVL2ï¼‰è¿›è¡Œå¾®è°ƒï¼Œä»…åœ¨ä»¿çœŸæ•°æ®ä¸‹æµ‹è¯•ï¼ŒSRä¸º70+%ï¼Œåœ¨æ–°ç¯å¢ƒä¸‹ä¸º50+% |
|2026|`arXiv`<br>ByteDance Seed|[VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory](https://arxiv.org/pdf/2601.08665)|[website](https://wsakobe.github.io/VLingNav-web/)|é€šè¿‡AdaCoTè‡ªé€‚åº”ç”Ÿæˆè¯­è¨€åŒ–æ€ç»´é“¾ï¼ˆå®ç°æ˜¾å¼æ¨ç†ï¼‰+VLingMemå®ç°åŸºäºå…³é”®è§†è§‰ç‰¹å¾çš„è®°å¿†å­˜å‚¨ï¼ˆforé•¿ç¨‹ä»»åŠ¡ï¼Œè¯­è¨€ï¼‰ï¼ŒåŸºäºè¯­è¨€é©±åŠ¨çš„è®¤çŸ¥å†é€šè¿‡VLAï¼ˆLLaMA-7B+ViT-Lï¼Œ+online RLï¼‰å®ç°å…·èº«å¯¼èˆªï¼›æ­¤å¤–ï¼Œæ„å»ºäº†Nav-AdaCoT-2.9Må…·èº«å¯¼èˆªæ•°æ®é›†ï¼ˆå¸¦æœ‰æ¨ç†è¿‡ç¨‹ï¼Œé‡‡ç”¨åŸºäºQwen 2.5 VL-72Bï¼‰ï¼›128ä¸ªA100 GPUè®­ç»ƒï¼Œç›®æ ‡å¯¼èˆªSRä¸º58~83%ï¼ŒçœŸæœºéƒ¨ç½²ä¸ºUnitree Go2+RTX 4090 GPUï¼Œé€šä¿¡å»¶è¿Ÿçº¦100msï¼Œæ¨ç†é€Ÿåº¦çº¦2.5 FPS|
|2026|`AAAI`<br>Chinese Academy of Sciences|[UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories](https://arxiv.org/pdf/2512.09607)|[![Github stars](https://img.shields.io/github/stars/CASIA-IVA-Lab/UrbanNav.svg)](https://github.com/CASIA-IVA-Lab/UrbanNav)|æ•°æ®é›†UrbanNavï¼Œç¬¬ä¸€äººç§°è§†è§’çš„å¹²å‡€ã€å®‰å…¨çš„è§†è§‰â€“è¯­è¨€â€“è½¨è¿¹ä¸‰å…ƒç»„æ•°æ®é›†{é‡Œç¨‹è®¡ç”¨DPVOè·å–ï¼Œå‰”é™¤è§†è§’ä¸åŠ¨ä½œå¼±å¯¹é½æˆ–å­˜åœ¨å®‰å…¨éšæ‚£çš„è¡Œä¸ºï¼ˆå¦‚ç§»åŠ¨ä¸­é¢‘ç¹è½¬å¤´ã€åœ¨å¯†é›†äººæµä¸­ç©¿è¡Œç­‰ï¼‰ï¼ŒåŸºäºæ£€æµ‹åˆ°çš„åœºæ™¯ç‰©ä½“åŠå…¶ç©ºé—´ä½ç½®ï¼Œé‡‡ç”¨ Qwen2.5-VL-72B è‡ªåŠ¨ç”Ÿæˆä¸è½¨è¿¹è¯­ä¹‰å¯¹é½çš„è‡ªç„¶è¯­è¨€å¯¼èˆªæŒ‡ä»¤}ï¼›æ¶æ„ï¼šåˆ©ç”¨CLIPç¼–ç è¯­è¨€æŒ‡ä»¤ã€DINOv2æå–è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡FiLMæœºåˆ¶å®ç°è¯­è¨€å¯¹è§†è§‰è¡¨å¾çš„åŠ¨æ€è°ƒåˆ¶ï¼Œå†ç»æ—¶ç©ºTransformerå»ºæ¨¡è·¨æ¨¡æ€äº¤äº’ä¸å†å²ä¸Šä¸‹æ–‡|
|2026|`arXiv`<br>Zhejiang University|[VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments](https://arxiv.org/pdf/2512.15258)|---|1. åŸºäº3D-GSçš„é«˜ä¿çœŸæ•°æ®æ„å»º(100K+ è½¨è¿¹å’Œ 1M+ å¤šæ¨¡æ€æ ·æœ¬çš„æ··åˆæ•°æ®é›†);2.æ¸è¿›å¼ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥(æµ·é‡VQAæ•°æ®è®­ç»ƒå­¦ä¼šçœ‹å›¾è¯´è¯/ç©ºé—´æ¨ç†---å¯¼èˆªè®­ç»ƒå­¦ä¹ è¾“å‡º3Dèˆªç‚¹å’Œåèˆªè§’---å¼ºåŒ–å­¦ä¹ ä¿è¯é•¿åºåˆ—ä»»åŠ¡çš„å†³ç­–ä¸€è‡´æ€§å’Œé²æ£’æ€§)ï¼›3. éç”Ÿæˆå¼åŠ¨ä½œæ¨¡å—ï¼ˆä¸ºäº†ä¿è¯å®‰å…¨ä¸ç›´æ¥è¾“å‡ºç”µæœºæ§åˆ¶ä¿¡å·ï¼Œè€Œæ˜¯è¾“å‡ºå±€éƒ¨3Dèˆªç‚¹ï¼‰+è½»é‡çº§çš„action moduleï¼ˆç»“åˆæ·±åº¦ä¿¡æ¯ä¿è¯å®‰å…¨ï¼‰+è½»é‡åŒ–æ¨¡å‹ï¼ˆFlash-Attentionã€ç®—å­èåˆã€KV-Cache é¢„åŠ è½½ä»¥åŠ CUDA Graph è°ƒåº¦ï¼‰ã€‚7Bæ¨¡å‹ï¼ˆViT+LLMï¼‰ï¼Œåœ¨Jetson Orin NXï¼ˆ 100+TOPSï¼‰ä¸Šå¯å®ç°2ï½3Hzçš„æ§åˆ¶é¢‘ç‡ï¼ˆ7Bæ¨¡å‹ï¼š0.11s/tokenï¼Œ 2Bæ¨¡å‹ï¼š0.032s/tokenï¼‰ï¼Œå•ä»»åŠ¡æˆåŠŸç‡é«˜è¾¾98%ä¸”æœ‰å®æœºé£è¡Œæµ‹è¯•|
|2026|`RAL`<br>HKUST-GZ|[Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration](https://arxiv.org/pdf/2505.23019)|[![Github stars](https://img.shields.io/github/stars/Zeying-Gong/ascent.svg)](https://github.com/Zeying-Gong/ascent)<br>[website](https://zeying-gong.github.io/projects/ascent/)|è·¨æ¥¼å±‚ç‰©ä½“ç›®æ ‡å¯¼èˆªï¼›å¤šæ¥¼å±‚æŠ½è±¡ï¼ˆæ¯å±‚æ¥¼ç»´æŠ¤ç‹¬ç«‹çš„é¸Ÿç°å›¾è¡¨ç¤ºï¼‰ã€ç²—åˆ°ç²¾åˆ†å±‚æ¨ç†ï¼šVLMå¯¹å‰æ²¿ç‚¹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æ’åºï¼Œå¿…è¦æ—¶æ‰è°ƒç”¨LLMè¿›è¡Œæ·±åº¦æ¨ç†ï¼ˆåŸºäºæ¥¼å±‚å…ˆéªŒæ¦‚ç‡å†³å®šæ˜¯å¦åˆ‡æ¢æ¥¼å±‚ã€åŸºäºåŒºåŸŸå…ˆéªŒæ¦‚ç‡é€‰æ‹©æœ€ç›¸å…³çš„æ¢ç´¢ç›®æ ‡ï¼‰ï¼›åœ¨HM3Dä¸ŠSRä¸º65.4%ï¼ŒMP3Dä¸º44.5%ï¼›Unitree Go2 ä¸ŠçœŸæœºéªŒè¯ï¼Œæ ¸å¿ƒç®—æ³•åœ¨ç¬”è®°æœ¬RTX2060ï¼ŒLLMåœ¨RTX3090æœåŠ¡å™¨|
|2025|`ICRA`<br>KTH Royal Institute of Technology|[One map to find them all: Real-time open-vocabulary mapping for zero-shot multi-object navigation](https://arxiv.org/pdf/2409.11764?)|[![Github stars](https://img.shields.io/github/stars/KTH-RPL/OneMap.svg)](https://github.com/KTH-RPL/OneMap)<br> [website](https://www.finnbusch.com/OneMap/)|OneMapé€šè¿‡æ„å»ºä¸€ä¸ªå¯å¤ç”¨çš„ã€å®æ—¶çš„ã€å¼€æ”¾è¯æ±‡ï¼ˆOpen-Vocabularyï¼‰è¯­ä¹‰åœ°å›¾ï¼›é›¶æ ·æœ¬ç‰©ä½“å¯¼èˆªï¼›å›¾åƒçº§çš„ CLIP ç‰¹å¾+ SED (Simple Encoder-Decoder) æ¶æ„ï¼Œåˆ©ç”¨å¡å°”æ›¼æ»¤æ³¢å°†å¸¦æœ‰æ–¹å·®çš„ç‰¹å¾èåˆåˆ°2Dæ …æ ¼åœ°å›¾ä¸Šï¼›è‡³äºå¯¼èˆªä¸æœç´¢ç­–ç•¥ï¼šæœºå™¨äººä¼˜å…ˆå‰å¾€è¯­ä¹‰ç›¸ä¼¼åº¦é«˜ä¸”æœªè¢«å½“å‰ä»»åŠ¡æœç´¢è¿‡çš„åŒºåŸŸï¼›HM3D æ•°æ®é›†æˆåŠŸç‡ä¸º55.8%ï¼›å¯ä»¥åœ¨ Jetson Orin AGX ä¸Šï¼ˆBoston Dynamics Spot+Realsense D455 + Livox Lidarï¼‰å®æ—¶è¿è¡Œï¼ˆ2HZï¼‰|
|2025|`arXiv`<br>Tencent AI Lab|[SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization](https://arxiv.org/pdf/2512.02631)|[![Github stars](https://img.shields.io/github/stars/WzcTHU/SeeNav-Agent.svg)](https://github.com/WzcTHU/SeeNav-Agent)<br>[website](https://huggingface.co/wangzc9865/SeeNav-Agent)|åŒè§†è§’æç¤ºè¯ï¼ˆç¬¬ä¸€äººç§°è§†è§’+ä¿¯è§†å›¾ï¼ŒæŠŠè·¯æ ‡ã€åŠ¨ä½œ/å¯¼èˆªçº¿ç›´æ¥ç”»åœ¨å›¾ä¸Šï¼‰+RL;åœ¨EmbodiedBench Navigation åŸºå‡†ä¸ŠGPT-4.1 +è§†è§‰æç¤ºæˆåŠŸç‡è¾¾86.7%ï¼Œç›¸æ¯”åŸºåº§æ¨¡å‹æå‡äº† 21.7%ï¼Œæ¯”ä¹‹å‰çš„é—­æº SOTAï¼ˆClaude-3.5-Sonnetï¼‰é«˜å‡ºäº† 20 ä¸ªç™¾åˆ†ç‚¹ï¼›ä½¿ç”¨å¼€æºæ¨¡å‹ Qwen2.5-VL-3Bï¼ŒåŠ ä¸ŠRLå¾®è°ƒï¼Œæ¯”ä¹‹å‰çš„å¼€æº SOTAï¼ˆInternVL3-78Bï¼‰é«˜å‡º 5.6%| 
|2025|`arXiv`<br>Tsinghua University|[Nav-R2: Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation](https://arxiv.org/pdf/2512.02400)|[![Github stars](https://img.shields.io/github/stars/AMAP-EAI/Nav-R2.svg)](https://github.com/AMAP-EAI/Nav-R2)|é¢å‘å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªï¼ˆObject-goal navigationï¼‰ï¼Œæ¶æ„ä¸ºLLMä¸»å¹²ï¼ˆQwen2.5-VL-7Bï¼‰+è§†è§‰ç¼–ç å™¨+è®°å¿†æ¨¡å—ï¼›æ„å»ºäº† Chain-of-Thoughtï¼ˆCoTï¼ŒåŒ…å«äº†ç›®æ ‡-ç¯å¢ƒå»ºæ¨¡ä¸ç¯å¢ƒ-åŠ¨ä½œè§„åˆ’ï¼‰æ•°æ®é›†ï¼ˆ Qwen2.5-VL-7B ç”Ÿæˆæ ‡æ³¨ï¼‰ï¼Œä»…åœ¨æ¨¡æ‹Ÿæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæ”¯æŒ2HZçš„å®æ—¶æ¨ç†ï¼›HM3D-OVONæ•°æ®é›†çš„æˆåŠŸç‡ä¸º44~45%|
|2025|`arXiv`<br>Amap|[NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction](https://arxiv.org/pdf/2512.01550)|---|é¦–æ¬¡å°†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è§„åˆ’(å°†å¤æ‚æŒ‡ä»¤åˆ†è§£ä¸ºå­æŒ‡ä»¤)ä¸ä¸–ç•Œæ¨¡å‹é¢„æµ‹ï¼ˆé¢„æµ‹ç¯å¢ƒç‰¹å¾æ¥æŒ‡å¯¼å¯¼èˆªï¼‰æ•´åˆåˆ°ç»Ÿä¸€æ¡†æ¶ä¸‹ï¼Œç”¨äºå¯¼èˆªä»»åŠ¡ï¼›VLMè§„åˆ’è®­ç»ƒï¼ˆGemini 2.5Proæ„å»ºå±‚çº§è¯­è¨€è§„åˆ’/Hierarchical Language planningæ•°æ®é›†ï¼‰+ä¸–ç•Œæ¨¡å‹è®­ç»ƒï¼ˆDual-Horizon Predictive Foresightï¼Œé€šè¿‡decoderé¢„æµ‹æœªæ¥å‡ æ­¥çš„ç¯å¢ƒç‰¹å¾ï¼Œå¦‚DINOV2ï¼ŒSAMï¼Œå¸®åŠ©æ™ºèƒ½ä½“å¢å¼ºå±€éƒ¨æ„ŸçŸ¥ã€é¿éšœï¼‰ã€‚æ¨¡å‹æ¶æ„åŸºäº Qwen2.5-VL-3B-Instruct æ„å»ºï¼ˆåŒæ—¶å®ç°è§„åˆ’å’Œé¢„æµ‹ï¼‰ï¼ŒåŒ…å«æ–‡æœ¬ç¼–ç å™¨ã€å›¾åƒç¼–ç å™¨å’Œä½ç½®ç¼–ç å™¨ï¼›åœ¨ R2R-CE å’Œ RxR-CE æ•°æ®é›†ä¸ŠSR>66%|
|2025|`arXiv`<br>Nanjing University|[LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments](https://arxiv.org/pdf/2510.19655)|[website](https://robo-lavira.github.io/lavira-zs-vln/)|åˆ†å±‚å¯¼èˆªæ¡†æ¶ï¼šå¤§æ¨¡å‹å†³ç­–ï¼ˆGemini-2.5-Pro/GPT-4oï¼Œæ¥æ”¶è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€å†å²è½¨è¿¹å’Œå½“å‰å…¨æ™¯è§‚å¯Ÿã€‚å……å½“â€œå¤§è„‘â€ï¼Œè¾“å‡ºå®è§‚çš„æ–¹å‘æ€§æŒ‡ä»¤åŠè¿›åº¦çš„è¯­è¨€è¯„ä¼°ï¼‰+å°æ¨¡å‹å®šä½/æ„ŸçŸ¥ï¼ˆQwen2.5-VL-32Bï¼Œè¯†åˆ«ä¸æŒ‡ä»¤æœ€ç›¸å…³çš„ç‰©ä½“/åŒºåŸŸï¼Œå¹¶è¾“å‡ºå…¶è¾¹ç•Œæ¡†ï¼‰+è§„åˆ’æ§åˆ¶ï¼ˆå°†è§†è§‰åŠ¨ä½œè¾“å‡ºçš„ 2D ç›®æ ‡æŠ•å½±åˆ° 3D ä¸–ç•Œåæ ‡ç³»ä¸­ï¼Œç”Ÿæˆå±€éƒ¨åœ°å›¾ä¸Šçš„å¯¼èˆªç‚¹ï¼Œå¹¶é€šè¿‡è·¯å¾„è§„åˆ’é©±åŠ¨æœºå™¨äººç§»åŠ¨ï¼‰ï¼›VLN-CEä¸ŠæˆåŠŸç‡>35%ï¼›çœŸæœºæµ‹è¯•ï¼šUnitree Go1 å››è¶³æœºå™¨äººå’ŒAgilex Cobot Magic è½®å¼æœºå™¨äºº|
|2025|`arXiv`<br>HKUST-GZ|[PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory](https://arxiv.org/pdf/2511.06840v1)|---|Zero-shotç›®æ ‡å¯¼èˆªï¼Œæ— éœ€ prebuilt mapsï¼›å…¨æ™¯ï¼ˆ6ä¸ªæ–¹å‘ï¼‰RGBè¾“å…¥MLLMï¼ˆQwen2.5-VLï¼‰ï¼Œæ­¤å¤–ï¼Œç»“åˆå½“å‰çš„å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ä»¥åŠå†å²è®°å¿†ä¿¡æ¯ï¼ˆå­˜å‚¨åœ¨åŠ¨æ€æœ‰ç•Œçš„è®°å¿†é˜Ÿåˆ—ï¼‰åˆ©ç”¨LLMï¼ˆDeepSeek-V3ï¼‰åšå¯¼èˆªå†³ç­–ï¼ˆå†³ç­–ç»“æœåŒ…æ‹¬å¯¼èˆªæ–¹å‘å’Œæ˜¯å¦æ‰¾åˆ°ç›®æ ‡çš„æ ‡å¿—ï¼‰ï¼Œåœ¨HM3Dæ•°æ®é›†ä¸ŠSRçº¦ä¸º43.5%|
|2025|`arXiv`<br>Peking University|[MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/pdf/2511.17889)|[![Github stars](https://img.shields.io/github/stars/AIGeeksGroup/MobileVLA-R1.svg)](https://github.com/AIGeeksGroup/MobileVLA-R1)<br>[website](https://aigeeksgroup.github.io/MobileVLA-R1/)<br>[Dataset](https://huggingface.co/datasets/AIGeeksGroup/MobileVLA-CoT)|ç»“æ„åŒ–çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰å®ç°è¯­ä¹‰æ¨ç†å’ŒæŒ‡ä»¤æ§åˆ¶å¯¹é½ï¼ˆGemini-2.5-Flashï¼Œé€šè¿‡æç¤ºè¯ï¼Œå°†RGB-Depthã€è¯­è¨€æŒ‡ä»¤ã€çŠ¶æ€åŠ¨ä½œå†å²ä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸ºæ§åˆ¶å‘½ä»¤ï¼‰ï¼›æå‡ºå¤šç²’åº¦çš„CoTå¯¹é½è¯­æ–™åº“ï¼šMobileVLA-CoTæ•°æ®é›†ï¼ŒåŒ…å«18Kä¸ªå‰§é›†çº§æ ·æœ¬ã€78Kä¸ªæ­¥éª¤çº§æ ·æœ¬å’Œä¸€ä¸ªåŒ…å«38K CoTæ³¨é‡Šçš„å¯¼èˆªå­é›†ï¼›é‡‡ç”¨CoTæ•°æ®é›†è¿›è¡Œç›‘ç£å¾®è°ƒå¯¹é½åŠRLè¿›è¡Œè®­ç»ƒï¼›MobileVLA-R1ä»¥NaVILAæ¶æ„ä¸ºåŸºç¡€ï¼Œå¼•å…¥æ·±åº¦å’Œç‚¹äº‘ç¼–ç å™¨ï¼Œå°†å¤šæ¨¡æ€ç‰¹å¾é€šè¿‡è½»é‡çº§æŠ•å½±æ¨¡å—èåˆï¼Œç„¶åä¸ LLaMA3-8B è¯­è¨€æ¨¡å‹å¯¹é½ï¼›è¯„ä¼°ä¸»è¦é‡‡ç”¨VLN-CEï¼ˆSRï¼š66~68%ï¼‰å’Œå››è¶³æœºå™¨äººçš„QUARDæ•°æ®é›†ï¼ˆå¹³å‡SR73%ï¼‰ï¼›çœŸæœºä¸ºUnitree Go2 ï¼ˆ L2 LiDARã€RGB-D ç›¸æœºå’ŒJetson Orin ï¼‰ |
|2025|`arXiv`<br>SRI International|[SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation](https://arxiv.org/pdf/2505.13729?)|---|åˆ©ç”¨LLMå®ç°å¼‚æ„å¤šæœºå™¨äººåä½œå¯¼èˆªï¼Œé¡¶å±‚å…¨å±€è§„åˆ’å™¨é‡‡ç”¨LLMç”Ÿæˆåä½œç­–ç•¥ï¼›ä¸­å±‚å±€éƒ¨è§„åˆ’å™¨åˆ©ç”¨ LLM ä¸ºæ¯ä¸ªæœºå™¨äººç”ŸæˆåŠ¨æ€çš„ã€é€æ­¥çš„è¡ŒåŠ¨è®¡åˆ’ï¼›åº•å±‚åŠ¨ä½œè§„åˆ’å™¨å°†ä¸­å±‚å±€éƒ¨è§„åˆ’å™¨ç”Ÿæˆçš„é€æ­¥è®¡åˆ’æ˜ å°„ä¸ºå…·ä½“çš„æœºå™¨äººåŠ¨ä½œåºåˆ—ï¼›graph generatorè´Ÿè´£åŸºäºæœºå™¨äººæ„ŸçŸ¥çš„ RGBD å›¾åƒæ„å»ºå’Œæ‰©å±•å±€éƒ¨ 3D åœºæ™¯å›¾|
|2025|`ICRA`<br>Chinese Academy of Sciences|[MiniVLN: Efficient Vision-and-Language Navigation by Progressive Knowledge Distillation](https://arxiv.org/pdf/2409.18800)|---|ç”¨è’¸é¦çš„æ–¹å¼ï¼ˆtwo-stage distillationï¼‰ï¼Œåœ¨ScaleVLNä»…1/7çš„å¤§å°ä¸‹ï¼Œè¾¾åˆ°äº†ç›¸åŒçš„å‡†ç¡®ç‡ï¼ˆREVERIEä¸Š>50%,R2Rä¸Š>70%ï¼‰|
|2025|`ICRA`<br> The University of Adelaide|[Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2502.19024)|---|ï¼ˆGVNavï¼‰ä¾é è°ƒå–ä½è§†è§’æœºå™¨äººï¼ˆæ¯”å¦‚æœºå™¨ç‹—ï¼Œæ‰«åœ°æœºå™¨äººç­‰ï¼‰åœ¨ä¸åŒé«˜åº¦ä¸‹è·å¾—çš„è§†è§‰ä¿¡æ¯ï¼Œæ¥è¡¥å…¨å½“å‰è§†è§’ï¼›é‡‡ç”¨çš„ScaleVLNï¼Œä»¿çœŸç¯å¢ƒæˆåŠŸç‡ä¸º50-60%ï¼Œæœºæ¢°ç‹—çœŸæœºæˆåŠŸç‡28-48%| 
|2025|`ICRA`<br>The University of Adelaide|[Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs](https://arxiv.org/pdf/2409.18794)|[![Github stars](https://img.shields.io/github/stars/YanyuanQiao/Open-Nav.svg)](https://github.com/YanyuanQiao/Open-Nav)<br>[website](https://sites.google.com/view/opennav)|åˆ©ç”¨å¼€æºçš„LLM(æœ¬åœ°éƒ¨ç½²éä¸ºcall API)å®ç°è¿ç»­ç¯å¢ƒä¸‹çš„zero-shot VLNï¼›Waypoint Prediction moduleè´Ÿè´£é¢„æµ‹æ½œåœ¨å¯å¯¼èˆªç‚¹ï¼Œåœºæ™¯æ„ŸçŸ¥æ¨¡å—åŒ…å«äº†RAMï¼ˆç‰©ä½“è¯†åˆ«ï¼‰å’ŒSpatialBotï¼ˆVLMæ¨¡å‹ï¼Œè¾“å…¥RGBå’Œæ·±åº¦ä¿¡æ¯ï¼Œåšç©ºé—´æ„ŸçŸ¥ï¼Œè¾“å‡ºä¸ºæ–‡æœ¬æè¿°ï¼‰ï¼›LLM Navigatoræ¥æ”¶å‰ä¸¤è€…çš„è¾“å…¥ï¼Œå°†ä»»åŠ¡åˆ†è§£ä¸ºï¼šæŒ‡ä»¤ç†è§£ã€è¿›åº¦ä¼°è®¡ã€å†³ç­–ï¼›çœŸæœºå®éªŒä¸ºè½®å¼æœºå™¨äººï¼ˆRTX 3080 GPUï¼‰ï¼›æµ‹è¯•äº†å››ä¸ªä¸åŒæ¨¡å‹ï¼ˆLlama3.1-70B-instruct, Qwen2-72Binstruct, Gemma2-27B-instruct and Phi3-14B-instructï¼‰æˆåŠŸç‡<20%|
|2025|`arXiv`<br>Adelaide University|[VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation](https://arxiv.org/pdf/2512.19021)|[![Github stars](https://img.shields.io/github/stars/william13077/IAmGoodNavigator.svg)](https://github.com/william13077/IAmGoodNavigator) <br>[website](https://sihaoevery.github.io/vlnverse/)|åŸºäº NVIDIA Isaac Sim æ„å»ºï¼ŒåŒ…å« 263 ä¸ªé«˜ä¿çœŸç‰©ç†åœºæ™¯ï¼Œå¹¶ä¸”ç»Ÿä¸€äº†ç»†ç²’åº¦ (Fine-grainedï¼Œä¸€æ­¥ä¸€æŒ‡ä»¤)ã€ç²—ç²’åº¦ (Coarse-grainedï¼Œç›®æ ‡å¯¼å‘)ã€è§†è§‰å‚è€ƒ (Visual-Referenceï¼Œçœ‹å›¾æ‰¾ç‰©)ã€é•¿ç¨‹ (Long-Horizonï¼Œå¤šé˜¶æ®µè¿ç»­å¯¼èˆª) åŠå¯¹è¯å¼ (Dialogue-basedï¼Œé€šè¿‡äº¤äº’è§£å†³æ­§ä¹‰) äº”å¤§å¯¼èˆªä»»åŠ¡ï¼›ä¸ä»…ä»…æ˜¯è§†è§‰ä¸Šçš„é«˜ä¿çœŸï¼Œæ¯ä¸ªåœºæ™¯ä¸­çš„ç‰©ä½“å‡å¯ç§»åŠ¨ï¼Œäº¤äº’ã€‚ç‰©ä½“çš„ç‰©ç†å±æ€§ï¼Œæ¯”å¦‚è´¨é‡ï¼Œæ‘©æ“¦ç³»æ•°ï¼Œåå…‰ç³»æ•°ä¹Ÿéƒ½æœ‰æä¾›ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æä¾›äº†è¯¦ç»†çš„æ‹“æ‰‘å’Œè¯­ä¹‰æ ‡æ³¨ï¼Œä»¥åŠoccupancy mapã€‚|
|2025|`arXiv`<br>Shanghai AI Laboratory|[Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation](https://arxiv.org/pdf/2512.08186)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternNav.svg)](https://github.com/InternRobotics/InternNav)<br>[website](https://internrobotics.github.io/internvla-n1-dualvln.github.io/)|ï¼ˆDualVLNï¼‰VLN é¢†åŸŸé¦–ä¸ªåŒç³»ç»ŸåŸºç¡€æ¨¡å‹ï¼›System2ï¼šåŸºäºQwen-VL-2.5(7B)çš„å…¨å±€è§„åˆ’å™¨ï¼Œä»¥çº¦ 2 Hz çš„é¢‘ç‡è¿è¡Œï¼Œè´Ÿè´£ç†è§£æŒ‡ä»¤ã€è§‚å¯Ÿç¯å¢ƒï¼Œå¹¶é¢„æµ‹åƒç´ çº§ç›®æ ‡ç‚¹ï¼ˆæ­¤å¤–ï¼ŒSystem2è¿˜å¯ä»¥è‡ªä¸»è°ƒæ•´è§†è§’ï¼‰ï¼›System1ï¼šè½»é‡çº§Diffusion Transformerï¼Œ30HZè¿è¡Œï¼Œæ¥æ”¶System2çš„åƒç´ ç›®æ ‡åŠéšå«çš„è¯­ä¹‰ç‰¹å¾ï¼Œç»“åˆå½“å‰é«˜é¢‘RGBå›¾åƒï¼Œç”Ÿæˆå¹³æ»‘ã€è¿ç»­ã€é¿éšœçš„è½¨è¿¹ï¼›çœŸæœºå®éªŒï¼šè½®å¼ï¼ˆTurtlebot4ï¼‰ã€å››è¶³ï¼ˆUnitree Go2ï¼‰ã€äººå½¢ï¼ˆUnitree G1ï¼‰ï¼Œå‡ä»…æ­è½½RealSense D455 å•ç›® RGB ç›¸æœº(æˆåŠŸç‡ï¼šR2R 64.3%ï¼ŒRxR 61.4%ï¼›ï¼‰|
|2025|`arXiv`<br>Peking University|[MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning](https://arxiv.org/pdf/2510.03142v1)|[website](https://pku-epic.github.io/MM-Nav-Web/)|è®­ç»ƒ3ä¸ªRLä¸“å®¶ï¼ˆåˆ°è¾¾/Reachingã€æŒ¤å‹/Squeezingã€èº²é¿/Avoidingï¼‰å¹¶è¿›è¡Œåˆå§‹VLAï¼ˆSigLIP+Qwen2-7Bï¼‰å¾®è°ƒï¼Œç„¶åéƒ¨ç½²åˆ°ä»¿çœŸç¯å¢ƒï¼Œå†è¿›è¡Œåœ¨çº¿çš„æ•™å¸ˆ-å­¦ç”Ÿè®­ç»ƒï¼Œéƒ¨ç½²åˆ°Unitree GO2ï¼ˆæ¨¡å‹è¿è¡Œåœ¨RTX5090ï¼‰ï¼Œ4ä¸ªé±¼çœ¼ç›¸æœºï¼ŒVKAè¾“å‡ºçš„é€Ÿåº¦ç”±åº•å±‚æ§åˆ¶å™¨æ‰§è¡Œï¼ˆå¹³å‡å“åº”ä¸º7HZï¼‰ï¼Œåˆ°è¾¾çš„æˆåŠŸç‡>80%ï¼Œæ··åˆä¸‰ç§æƒ…å†µæˆåŠŸç‡>47%|
|2025|ManifoldTechLtd|Odin-Nav-Stack|[![Github stars](https://img.shields.io/github/stars/ManifoldTechLtd/Odin-Nav-Stack.svg)](https://github.com/ManifoldTechLtd/Odin-Nav-Stack)<br>[website](https://manifoldtechltd.github.io/Odin-Nav-Stack-Webpage/)<br>[Blog](https://kwanwaipang.github.io/Odin-Navigation-Stack/)|ï¼ˆModular learning approachesï¼‰SLAM+åŠ¨æ€é¿éšœ/Neupan(åŸºäºé›·è¾¾ç‚¹è¾“å…¥çš„ç«¯åˆ°ç«¯å¯¼èˆªæ§åˆ¶)+è¯­ä¹‰å¯¼èˆª(åŸºäºæŒ‡ä»¤åˆ†è§£ä¸YoLoå®ç°ç‰©ä½“çš„æ£€æµ‹ã€å®šä½ä¸ç›¸å¯¹ç‰©ä½“çš„å¯¼èˆª)+åœºæ™¯æè¿°|
|2025|`arXiv`<br>The University of Hong Kong|[VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.17221)| [![Github stars](https://img.shields.io/github/stars/Qi-Zhangyang/GPT4Scene-and-VLN-R1.svg)](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1)<br>[website](https://vlnr1.github.io/)|åˆ©ç”¨å¤§å‹VLMï¼ˆQwen2-VLï¼‰å°†è§†é¢‘æ•°æ®æµè½¬æ¢ä¸ºè¿ç»­å¯¼èˆªactionï¼›Supervised fine-tuning(SFTï¼Œå®ç°action textçš„é¢„æµ‹)+Reinforcement fine-tuning (RFTï¼Œå¤šæ­¥é¢„æµ‹)ï¼›å¯¹äºè§†é¢‘è¾“å…¥é‡‡ç”¨Long-Short Memory Samplingç­–ç•¥ï¼Œå¹³è¡¡å†å²å¸§æ•°ä¸å®æ—¶æ€§ï¼ˆæ¨¡æ‹Ÿå™¨ä¸Šå¯¼èˆªæˆåŠŸç‡<35%ï¼‰|
|2025|`arXiv`<br>Horizon Robotics|[FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph](https://arxiv.org/pdf/2509.13733)| [![Github stars](https://img.shields.io/github/stars/HorizonRobotics/HoloAgent.svg)](https://github.com/HorizonRobotics/HoloAgent) <br>[website](https://horizonrobotics.github.io/robot_lab/fsr-vln/)|å±‚æ¬¡åŒ–å¤šæ¨¡æ€åœºæ™¯å›¾(HMSG)ï¼šå¤šæ¨¡æ€åœ°å›¾è¡¨å¾ï¼ˆFAST-LIVO2ç­‰ï¼šå‡ ä½•+è¯­ä¹‰+æ˜¾å¼æ‹“æ‰‘å…³ç³»ï¼‰å®ç°ç²—ç•¥çš„room-levelå®šä½åˆ°ç²¾ç»†çš„ç›®æ ‡è§†è§’ä¸ç‰©ä½“å®šä½ï¼›æ¥ä¸‹æ¥çš„å¿«é€Ÿåˆ°æ…¢é€Ÿå¯¼èˆªæ¨ç†ï¼ˆFSRï¼‰åŸºäºHMGSï¼Œåº”ç”¨VLMå®ç°æœ€ç»ˆç›®æ ‡çš„é€‰æ‹©ï¼›åŸºäºé€‰æ‹©çš„ç›®æ ‡å®ç°è·¯å¾„è§„åˆ’ä»¥åŠå…¨èº«æ§åˆ¶æ¥åˆ°è¾¾ç›®æ ‡ï¼ˆè¿™éƒ¨åˆ†é‡‡ç”¨ä¼ ç»Ÿæ–¹æ¡ˆï¼‰ï¼›Unitree-G1éªŒè¯157må¯¼èˆª|
|2025|`arXiv`<br> University at Buffalo|[VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning](https://arxiv.org/pdf/2502.00931)|[website](https://sairlab.org/vlnav/)|é›†æˆäº†ç©ºé—´æ¨ç†ï¼ˆå°†è§†è§‰è¯­è¨€ç‰¹å¾è½¬æ¢ä¸ºç©ºé—´åˆ†æ•°åˆ†å¸ƒï¼‰ï¼Œåˆ†æ•°ï¼ˆCVL scoresï¼‰ä¸æ¯ä¸ªç›®æ ‡ç‚¹ç›¸ç»“åˆï¼Œå†é€šè¿‡curiosity-driven weightingå®ç°é€‰æ‹©ç›®æ ‡ç‚¹è¿›è¡Œæ¢ç´¢ï¼ˆä¿è¯æ‰€é€‰çš„ç›®æ ‡ç‚¹ä¸ä»…æ˜¯äººç±»æŒ‡ä»¤è¿˜æ˜¯è®©æœºå™¨äººæ¢ç´¢æœªçŸ¥åŒºåŸŸï¼‰,é€‰æ‹©ç›®æ ‡ç‚¹åé‡‡ç”¨ä¼ ç»Ÿçš„plannerè¿›è¡Œé¿éšœå¯¼èˆªï¼ˆModular learning approachesï¼‰ï¼›ç§»åŠ¨å°è½¦ï¼ˆOrin NXï¼‰ä¸Šå®ç°30HZé¢‘ç‡ï¼ˆæˆåŠŸç‡86.3%ï¼‰|
|2025|`arXiv`<br>Peking University|[Embodied navigation foundation model](https://arxiv.org/pdf/2509.12129)|[website](https://pku-epic.github.io/NavFoM-Web/)|(NavFoM)å…¨çƒé¦–ä¸ªè·¨æœ¬ä½“å…¨åŸŸç¯è§†å¯¼èˆªåŸºåº§å¤§æ¨¡å‹,æŠŠVLNã€Object-goal Navigationã€Visual Tracking å’ŒAutonomous Driving ç­‰ä¸åŒæœºå™¨äººçš„å¯¼èˆªä»»åŠ¡ç»Ÿä¸€åˆ°ç›¸åŒçš„èŒƒå¼,7Bå‚æ•°|
|2025|`arXiv`<br>Shanghai AI Laboratory|[InternVLA-N1: An Open Dual-System Vision-Language Navigation Foundation Model with Learned Latent Plans](https://internrobotics.github.io/internvla-n1.github.io/static/pdfs/InternVLA_N1.pdf)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternNav.svg)](https://github.com/InternRobotics/InternNav)<br> [website](https://internrobotics.github.io/internvla-n1.github.io/) |é¦–ä¸ªå¼€æºçš„åŸºäºåŒç³»ç»ŸVLNæ¨¡å‹:System 2 ç”¨äºæ‰§è¡Œå¤šè½®çš„åŸºäºè¯­è¨€æŒ‡ä»¤ã€è§‚æµ‹ï¼ˆè§†è§’æ„ŸçŸ¥ï¼‰çš„ç²¾ç¡®è§„åˆ’ã€‚è€ŒSystem1åˆ™æ˜¯è´Ÿè´£åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸‹ï¼Œæ‰§è¡ŒSystem2è¾“å‡ºçš„è§„åˆ’ã€‚å…¶ä¸­ï¼Œå­¦ä¹ åˆ°çš„latent plansä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæ¥å®ç°ä¸¤ä¸ªç³»ç»Ÿä¹‹é—´çš„äº¤äº’;æ­¤å¤–ï¼Œæå‡ºInternData-N1å¯¼èˆªæ•°æ®é›†ï¼ŒåŒ…å«äº†è¶…è¿‡3000ä¸ªåœºæ™¯çš„50 millionçš„å›¾åƒï¼Œä¸€å…±4,839å…¬é‡Œ|
|2025|`arXiv`|[Odyssey: Open-world quadrupeds exploration and manipulation for long-horizon tasks](https://arxiv.org/pdf/2508.08240)|[website](https://kaijwang.github.io/odyssey.github.io/)|---|
|2025|`arXiv`|[OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/pdf/2511.06182)|---|---|
|2025|`arXiv`<br>University of Texas at Austin|[VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/pdf/2509.18592?)|[![Github stars](https://img.shields.io/github/stars/VLN-Zero/vln-zero.github.io.svg)](https://github.com/VLN-Zero/vln-zero.github.io)<br>[website](https://vln-zero.github.io/)|é€šè¿‡VLMæ„å»ºsymbolic scene graphsï¼ˆç¬¦å·åœºæ™¯å›¾ï¼‰ï¼Œè¿›è€Œå¯ä»¥å®ç°zero-shotçš„neurosymbolic navigationï¼ˆç¥ç»ç¬¦å·å¯¼èˆªï¼‰|
|2026|`ICLR`<br> Amap Alibaba Group|[JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](https://arxiv.org/pdf/2509.22548)|[![Github stars](https://img.shields.io/github/stars/MIV-XJTU/JanusVLN.svg)](https://github.com/MIV-XJTU/JanusVLN)<br> [Website](https://miv-xjtu.github.io/JanusVLN.github.io/)|æ¨¡æ‹Ÿå¤§è„‘å·¦å³åŠçƒçš„åˆ†å·¥ï¼Œå¹¶è¡Œå¤„ç†â€œæ˜¯ä»€ä¹ˆâ€å’Œâ€œåœ¨å“ªé‡Œâ€çš„é—®é¢˜ï¼š2Dè§†è§‰è¯­ä¹‰ç¼–ç å™¨ (è¯­ä¹‰â€œå·¦è„‘â€ï¼ŒQwen2.5-VL)Â +3Dç©ºé—´å‡ ä½•ç¼–ç å™¨ (ç©ºé—´â€œå³è„‘â€ï¼ŒVGGT)ï¼›é€šè¿‡ä¸€ä¸ª3Dè§†è§‰åŸºç¡€æ¨¡å‹æ¥æ‰©å±•MLLM,å®ç°ä»ç©ºé—´å‡ ä½•ç¼–ç å™¨ä¸­è·å–3Då…ˆéªŒçŸ¥è¯†,è¿›è€Œå¢å¼ºæ¨¡å‹çš„ç©ºé—´ç†è§£èƒ½åŠ›;åŒéšå¼è®°å¿†(ç©ºé—´å‡ ä½•ä¸è§†è§‰è¯­ä¹‰è®°å¿†)çš„å†å²é”®å€¼å¯¹åˆ™æ˜¯é€šè¿‡3Dç©ºé—´å‡ ä½•ç¼–ç å™¨å’ŒMLLMçš„è¯­ä¹‰è§†è§‰ç¼–ç å™¨æ¥åˆ†åˆ«æå–;é€šè¿‡æ»‘åŠ¨çª—å£è¿›è¡ŒåŠ¨æ€åŠå¢é‡å¼æ›´æ–°|
|2025|`arXiv`<br>Shanghai AI Laboratory|[Streamvln: Streaming vision-and-language navigation via slowfast context modeling](https://arxiv.org/pdf/2507.05240)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/StreamVLN.svg)](https://github.com/InternRobotics/StreamVLN)<br> [Website](https://streamvln.github.io/)|å°†Video-LLMï¼ˆLLaVA-Videoæ¨¡å‹ï¼Œé‡‡ç”¨çš„æ˜¯Qwen2-7Bï¼‰æ‰©å±•ä¸ºäº¤é”™çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œè¿›è€Œå®ç°å¤šè½®å¯¹è¯ä¸‹ä¸è§†é¢‘çš„è¿ç»­äº¤äº’;ä¸ºäº†åº”å¯¹é•¿æœŸä¸Šä¸‹æ–‡ç®¡ç†å’Œè®¡ç®—æ•ˆç‡çš„æŒ‘æˆ˜ï¼ŒStreamVLNé‡‡ç”¨æ··åˆçš„æ…¢é€Ÿ-å¿«é€Ÿä¸Šä¸‹æ–‡å»ºæ¨¡ç­–ç•¥:å¿«é€Ÿæµå¼å¯¹è¯éƒ¨åˆ†é€šè¿‡æ´»åŠ¨å¯¹è¯çš„æ»‘åŠ¨çª—å£ä¿ƒè¿›å“åº”å¼åŠ¨ä½œç”Ÿæˆ;ç¼“æ…¢æ›´æ–°çš„å†…å­˜éƒ¨åˆ†ä½¿ç”¨3Dæ„ŸçŸ¥Tokenæ¥ä¿®å‰ªç­–ç•¥ä»¥åŠå‹ç¼©å†å²è§†è§‰çŠ¶æ€ã€‚è¿™éƒ¨åˆ†ä½¿å¾—StreamVLNå¯ä»¥é€šè¿‡é”®å€¼å¯¹çš„ç¼“å­˜é‡ç”¨ï¼Œå®ç°è¿è´¯çš„å¤šå›åˆå¯¹è¯|
|2025|`arXiv`|[GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/pdf/2509.10454)|[![Github stars](https://img.shields.io/github/stars/bagh2178/GC-VLN.svg)](https://github.com/bagh2178/GC-VLN)<br>[Website](https://bagh2178.github.io/GC-VLN/)|---|
|2025|`arXiv`|[Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting](https://arxiv.org/pdf/2509.20499)|---|---|
|2025|`arXiv`<br> HKUST-GZ|[SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning](https://arxiv.org/pdf/2509.20739)|---|ä¸ä¾èµ–äºSLAMçš„å¯¼èˆªæ¡†æ¶ã€‚é€šè¿‡VLMæ¥å®ç°åœºæ™¯ï¼ˆQwenï¼‰ä»¥åŠç‰©ä½“çº§åˆ«ï¼ˆGrounding DINOï¼‰çš„è¯­ä¹‰æ¨ç†,è¿›è€Œæ„å»ºè½»é‡çš„æ‹“æ‰‘åœ°å›¾ï¼ˆtopological representationsï¼‰ã€‚åŸºäºLLMï¼ˆGPT-4ï¼‰çš„å®ç°å­ç›®æ ‡çš„é€‰æ‹©ï¼Œè€ŒåŸºäºè§†è§‰çš„å±€éƒ¨è§„åˆ’ï¼ˆViplannerï¼‰å®ç°éšœç¢ç‰©èº²é¿ã€‚ æœ€åå†é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å®ç°è…¿å¼æœºå™¨äººçš„è¿åŠ¨æ§åˆ¶ã€‚|
|2025|`arXiv`<br>Robotics and AI Institute|[Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation](https://arxiv.org/pdf/2411.07848)|[website](https://sonia-raychaudhuri.github.io/nlslam/)<br>[Reading](https://kwanwaipang.github.io/nl-slam/)|LLMæ„å»ºè¯­è¨€å…ˆéªŒå› å­å›¾ä¸ä¼ ç»Ÿçš„SLAMå¯¼èˆªå› å­å›¾ç›¸ç»“åˆ| 
|2025|`RSS`<br>Peking University|[Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks](https://arxiv.org/pdf/2412.06224)|[![Github stars](https://img.shields.io/github/stars/jzhzhang/Uni-NaVid.svg)](https://github.com/jzhzhang/Uni-NaVid)<br>[website](https://pku-epic.github.io/Uni-NaVid/)|NaVidçš„æ‰©å±•ç‰ˆ,æ·»åŠ äº†tokenèåˆæœºåˆ¶å®ç°åŠ é€Ÿä»¥åŠæ‹“å±•VLNä»»åŠ¡ä¸º4ä¸ªå­ä»»åŠ¡(è§†è§‰è¯­è¨€å¯¼èˆªã€ç›®æ ‡å¯¼èˆªã€å…·èº«é—®ç­”ã€äººå‘˜è·Ÿéš)ï¼›æ¡†æ¶ä¸ºï¼šè§†è§‰ç¼–ç å™¨ï¼Œonline token merge mechanism ä»¥åŠLLMï¼ˆå…¸å‹çš„å°†è§†è§‰è§‚æµ‹ç¼–ç ä¸ºåºåˆ—è·Ÿè¯­è¨€tokenä¸€èµ·è¾“å…¥åˆ°LLMçš„æ¡†æ¶ï¼‰|
|2025|`RSS`<br>UC San Diego|[NaVILA: Legged Robot Vision-Language-Action Model for Navigation](https://arxiv.org/pdf/2412.04453)|[![Github stars](https://img.shields.io/github/stars/AnjieCheng/NaVILA.svg)](https://github.com/AnjieCheng/NaVILA)<br>[website](https://navila-bot.github.io/)<br>[Test](https://kwanwaipang.github.io/NaVILA/)|é‡‡ç”¨åŒç³»ç»Ÿæ¶æ„ï¼šVLM(è§†è§‰è¯­è¨€æ¨¡å‹)+locomotion policy (RLå®ç°çš„åŸºäºè§†è§‰çš„è¿åŠ¨æ§åˆ¶)ã€‚VLMå°†è¯­è¨€æŒ‡ä»¤å’Œå›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºmid-level action/åŠ¨ä½œè¯­è¨€æŒ‡ä»¤ï¼Œå†ç”±locomotion policyç¿»è¯‘ä¸ºæœºå™¨äººlow-level action/è¿æ§ç”µæœºæŒ‡ä»¤ï¼›|
|2025|`ICCV`|[Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation](https://arxiv.org/pdf/2507.04047)|[![Github stars](https://img.shields.io/github/stars/MTU3D/MTU3D.svg)](https://github.com/MTU3D/MTU3D)<br>[website](https://mtu3d.github.io/)|---| 
| 2025 | `ACL` | [MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation](https://arxiv.org/pdf/2502.13451) |---|---|
| 2025 | `CVPR` | [Scene Map-based Prompt Tuning for Navigation Instruction Generation](https://openaccess.thecvf.com/content/CVPR2025/papers/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.pdf) |---|---|
| 2025 | `ACL` | [NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM](https://arxiv.org/pdf/2502.11142) | [![Github stars](https://img.shields.io/github/stars/MrZihan/NavRAG.svg)](https://github.com/MrZihan/NavRAG) |---|
| 2025 | ICLR | [Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel](https://arxiv.org/abs/2412.08467) | [![Github stars](https://img.shields.io/github/stars/wz0919/VLN-SRDF.svg)](https://github.com/wz0919/VLN-SRDF) |---|
| 2025 | `ICCV` | [SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts](https://arxiv.org/pdf/2412.05552) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/SAME.svg)](https://github.com/GengzeZhou/SAME) |---|
| 2025 | `ICCV` | [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/pdf/2506.23468) | [![Github stars](https://img.shields.io/github/stars/Feliciaxyao/NavMorph.svg)](https://github.com/Feliciaxyao/NavMorph) |---|
| 2025 | `AAAI` | [Affordances-Oriented Planning using Foundation Models for Continuous Vision-Language Navigation](https://arxiv.org/abs/2407.05890) |---|---|
| 2025 | Arxiv | [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/pdf/2506.01551) | [![Github stars](https://img.shields.io/github/stars/expectorlin/EvolveNav.svg)](https://github.com/expectorlin/EvolveNav) |---|
| 2025 | `CVPR` | [Do Visual Imaginations Improve Vision-and-Language Navigation Agents?](https://arxiv.org/pdf/2503.16394) |---|---|
|2024|`arXiv`<br>New York University|[Orionnav: Online planning for robot autonomy with context-aware llm and open-vocabulary semantic scene graphs](https://arxiv.org/pdf/2410.06239)|[video](https://www.youtube.com/watch?v=gqWCSjLLG1E)|åœ¨çº¿è‡ªä¸»å¯¼èˆªæ¡†æ¶ï¼ˆå››è¶³æœºå™¨äººã€Jetson AGX Orinå’ŒJetson Orin Nanoã€ä¼ æ„Ÿå™¨ï¼šRGBDã€LiDARã€IMUã€è…¿éƒ¨é‡Œç¨‹è®¡ï¼‰ï¼šLiDAR-SLAMï¼ˆ2Dæ …æ ¼åœ°å›¾ä¸ºä¸»ï¼‰+å¼€æ”¾è¯æ±‡è¡¨3Dè¯­ä¹‰æ˜ å°„æ–¹æ³•ï¼ˆåŸºäºRGBDçš„è¯­ä¹‰ç‰©ä½“åœ°å›¾ï¼ŒFC-CLIPï¼‰+åŸºäºLLMï¼ˆGPT-4-Turbo,äº‘ç«¯APIè°ƒç”¨ï¼‰çš„è§„åˆ’å™¨+ROS2å¯¼èˆªstack+[mexplore ROS2](https://github.com/robo-friends/m-explore-ros2)æ¢ç´¢ï¼›96æ¬¡çœŸæœºå®éªŒä¸­æˆåŠŸå®Œæˆäº†85æ¬¡ï¼ŒæˆåŠŸç‡ä¸º88.5%|
| 2024 | `AAAI` | [VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation](https://arxiv.org/abs/2402.03561) |---|---|
| 2024 | `CVPR` | [Volumetric Environment Representation for Vision-Language Navigation](https://arxiv.org/pdf/2403.14158) | [![Github stars](https://img.shields.io/github/stars/DefaultRui/VLN-VER.svg)](https://github.com/DefaultRui/VLN-VER) |---|
|2024|`ECCV`<br>University of Adelaid|[Navgpt-2: Unleashing navigational reasoning capability for large vision-language models](https://arxiv.org/pdf/2407.12366?)|[![Github stars](https://img.shields.io/github/stars/GengzeZhou/NavGPT-2.svg)](https://github.com/GengzeZhou/NavGPT-2)|VLMå¤„ç†å›¾åƒå’ŒæŒ‡ä»¤ï¼Œç„¶åå°†VLMè·å–çš„è§†è§‰Tokenä½œä¸ºLLMï¼ˆFrozenï¼‰çš„è¾“å…¥ï¼Œç”±LLMç”Ÿæˆå¯¼èˆªæ¨ç†ï¼ŒLLMçš„æ¨ç†åˆ™è¿›ä¸€æ­¥è¾“å…¥åˆ°å¯¼èˆªç­–ç•¥ç½‘ç»œè¾“å‡ºaction| 
| 2024 | `CVPR` | [Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation](https://arxiv.org/pdf/2404.01943) | [![Github stars](https://img.shields.io/github/stars/MrZihan/HNR-VLN.svg)](https://github.com/MrZihan/HNR-VLN) |---|
| 2024 | `TPAMI` | [ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2304.03047v2) | [![Github stars](https://img.shields.io/github/stars/MarSaKi/ETPNav.svg)](https://github.com/MarSaKi/ETPNav) |---|
| 2024 | `MM` | [Narrowing the Gap between Vision and Action in Navigation](https://www.arxiv.org/abs/2408.10388) |---|---|
| 2024 | `ECCV` | [LLM as Copilot for Coarse-grained Vision-and-Language Navigation](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00833.pdf) |---|---|
| 2024 | `ICRA` | [Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions](https://ieeexplore.ieee.org/abstract/document/10611565) | [![Github stars](https://img.shields.io/github/stars/LYX0501/DiscussNav.svg)](https://github.com/LYX0501/DiscussNav) |---|
| 2024 | `ACL` | [MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation](https://arxiv.org/abs/2401.07314) | [![Github stars](https://img.shields.io/github/stars/chen-judge/MapGPT.svg)](https://chen-judge.github.io/MapGPT/) |---|
| 2024 |`arXiv`| [MC-GPT: Empowering Vision-and-LanguageNavigation with Memory Map and Reasoning Chains](https://arxiv.org/pdf/2405.10620) |---|---|
| 2024 |`arXiv`| [InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment](https://arxiv.org/pdf/2406.04882) | [![Github stars](https://img.shields.io/github/stars/LYX0501/InstructNav.svg)](https://github.com/LYX0501/InstructNav) |---|
| 2024 | `AAAI` | [NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models](https://arxiv.org/abs/2305.16986) | [![Github stars](https://img.shields.io/github/stars/GengzeZhou/NavGPT.svg)](https://github.com/GengzeZhou/NavGPT) |é‡‡ç”¨GPT-4ä½œä¸ºzero-shotå¯¼èˆªå™¨ï¼Œå°†è§†è§‰è§‚æµ‹è½¬æ¢ä¸ºæ–‡æœ¬æè¿°çš„å€™é€‰viewpointï¼Œç„¶åç”¨LLMå¤„ç†è¿›ä¸€æ­¥ç”Ÿæˆæ–‡æœ¬action|
| 2024 | NACCL Findings | [LangNav: Language as a Perceptual Representation for Navigation](https://aclanthology.org/2024.findings-naacl.60.pdf) | [![Github stars](https://img.shields.io/github/stars/pbw-Berwin/LangNav.svg)](https://github.com/pbw-Berwin/LangNav) |---|
| 2024 |`arXiv`| [NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning](https://arxiv.org/abs/2403.07376) | [![Github stars](https://img.shields.io/github/stars/expectorlin/NavCoT.svg)](https://github.com/expectorlin/NavCoT) |---|
| 2024 | `CVPR` | [Towards Learning a Generalist Model for Embodied Navigation](https://arxiv.org/abs/2312.02010) | [![Github stars](https://img.shields.io/github/stars/LaVi-Lab/NaviLLM.svg)](https://github.com/LaVi-Lab/NaviLLM) |NaviLLM|
| 2024 | `RSS`<br> Peking University | [NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation](https://arxiv.org/pdf/2402.15852) | [![Github stars](https://img.shields.io/github/stars/jzhzhang/NaVid-VLN-CE.svg)](https://github.com/jzhzhang/NaVid-VLN-CE)<br>[website](https://pku-epic.github.io/NaVid/) |é¦–ä¸ªæ¢ç´¢VLMï¼ˆLLaMA-VIDï¼‰æ˜¯å¦å¯ä»¥èƒœä»»VLNä»»åŠ¡ï¼Œæå‡ºç¬¬ä¸€ä¸ªvideo VLM-based navigation agentï¼›åœ¨è¿ç»­ç©ºé—´ä¸‹ç›´æ¥æ¨æ–­low-level action|
| 2024 |`EMNLP`| [Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation](https://arxiv.org/pdf/2409.17313) | [![Github stars](https://img.shields.io/github/stars/zehao-wang/navnuances.svg)](https://github.com/zehao-wang/navnuances) |---|
| 2023 | `CVPR` | [Behavioral Analysis of Vision-and-Language Navigation Agents](https://yoark.github.io/assets/pdf/vln-behave/vln-behave.pdf) | [![Github stars](https://img.shields.io/github/stars/Yoark/vln-behave.svg)](https://github.com/Yoark/vln-behave) |---|
| 2023 | `ICCV` | [March in Chat: Interactive Prompting for Remote Embodied Referring Expression](https://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_March_in_Chat_Interactive_Prompting_for_Remote_Embodied_Referring_Expression_ICCV_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YanyuanQiao/MiC.svg)](https://github.com/YanyuanQiao/MiC) |---|
| 2023 |`arXiv`| [Vision and Language Navigation in the Real World via Online Visual Language Mapping](https://arxiv.org/pdf/2310.10822) |---|---|
| 2023 | `NeurIPS` | [A2Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models](https://peihaochen.github.io/files/publications/A2Nav.pdf) |---|---|
| 2023 | `ICCV` | [BEVBert: Multimodal Map Pre-training for Language-guided Navigation](https://arxiv.org/pdf/2212.04385) | [![Github stars](https://img.shields.io/github/stars/MarSaKi/VLN-BEVBert.svg)](https://github.com/MarSaKi/VLN-BEVBert) |---|
|2023|`CVPR`<br>University of Washington|[Cows on pasture: Baselines and benchmarks for language-driven zero-shot object navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/real-stanford/cow.svg)](https://github.com/real-stanford/cow)<br>[website](https://cow.cs.columbia.edu/)|CLIP on Wheelsï¼Œå°†CLIPç”¨åˆ°VLNä»»åŠ¡ä¸­|
|2023|`NIPS`<br>Chinese Academy of Sciences|[Frequency-enhanced data augmentation for vision-and-language navigation](https://proceedings.neurips.cc/paper_files/paper/2023/file/0d9e08f247ca7fbbfd5e50b7ff9cf357-Paper-Conference.pdf)|[![Github stars](https://img.shields.io/github/stars/hekj/FDA.svg)](https://github.com/hekj/FDA)|å°†VLNçš„æ•°æ®å¢å¹¿é—®é¢˜ä»spatial domainçš„æ¢ç´¢è½¬ç§»åˆ°Fourier domain|
|2023|`NIPS`<br>Peking University|[Find what you want: Learning demand-conditioned object attribute space for demand-driven navigation](https://proceedings.neurips.cc/paper_files/paper/2023/file/34e278fbbd7d6d7d788c98065988e1a9-Paper-Conference.pdf)|[![Github stars](https://img.shields.io/github/stars/whcpumpkin/Demand-driven-navigation.svg)](https://github.com/whcpumpkin/Demand-driven-navigation)<br>[website](https://sites.google.com/view/demand-driven-navigation)|é¦–æ¬¡æå‡ºæå‡ºéœ€æ±‚é©±åŠ¨çš„å¯¼èˆªä»»åŠ¡ï¼ˆDDNï¼‰;é€šè¿‡ä»LLMï¼ˆGPT-3ï¼‰ä¸­æå–å¸¸è¯†çŸ¥è¯†æ¥è·å–å¯¹è±¡æ–‡æœ¬å±æ€§ç‰¹å¾ã€‚è€Œæ–‡æœ¬å±æ€§ç‰¹å¾è·Ÿè§†è§‰ç‰¹å¾é€šè¿‡CLIPæ¥å¯¹é½|
|2023|`ACL`<br>University of California|[Aerial vision-and-dialog navigation](https://arxiv.org/pdf/2205.12219)|[![Github stars](https://img.shields.io/github/stars/eric-ai-lab/Aerial-Vision-and-Dialog-Navigation.svg)](https://github.com/eric-ai-lab/Aerial-Vision-and-Dialog-Navigation)<br>[website](https://sites.google.com/view/aerial-vision-and-dialog/home)|ç±»ä¼¼äºCVDNçš„åŸºäºäººç±»å¯¹è¯çš„æ— äººæœºå¯¼èˆª| 
| 2023 | `AAAI` | [Actional Atomic-Concept Learning for Demystifying Vision-Language Navigation](https://arxiv.org/pdf/2302.06072) |---|---|
| 2023 | `ICCV` | [Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation](https://arxiv.org/abs/2308.12587) | [![Github stars](https://img.shields.io/github/stars/CSir1996/VLN-GELA.svg)](https://github.com/CSir1996/VLN-GELA) |---|
| 2023 | `CVPR` | [Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/chengaopro/AZHP.svg)](https://github.com/chengaopro/AZHP) |---|
| 2023 | `ICCV` | [Bird's-Eye-View Scene Graph for Vision-Language Navigation](https://arxiv.org/abs/2308.04758) |---|---|
| 2023 |`EMNLP`| [Masked Path Modeling for Vision-and-Language Navigation](https://arxiv.org/abs/2305.14268) |---|---|
| 2023 | `CVPR` | [Improving Vision-and-Language Navigation by Generating Future-View Image Semantics](https://arxiv.org/pdf/2304.04907) | [![Github stars](https://img.shields.io/github/stars/jialuli-luka/VLN-SIG.svg)](https://github.com/jialuli-luka/VLN-SIG) |---|
| 2023 | `TPAMI` | [HOP+: History-Enhanced and Order-Aware Pre-Training for Vision-and-Language Navigation](https://ieeexplore.ieee.org/document/10006384) |---|---|
| 2023 | `TPAMI` | [Learning to Follow and Generate Instructions for Language-Capable Navigation](https://ieeexplore.ieee.org/document/10359152) |---|---|
| 2023 | `CVPR` | [A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning](https://arxiv.org/pdf/2210.03112) |[Dataset](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5)|---|
| 2023 | `CVPR` | [Lana: A Language-Capable Navigator for Instruction Following and Generation](https://arxiv.org/abs/2303.08409) | [![Github stars](https://img.shields.io/github/stars/wxh1996/LANA-VLN.svg)](https://github.com/wxh1996/LANA-VLN) |---|
| 2023 | `CVPR` | [KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_KERM_Knowledge_Enhanced_Reasoning_for_Vision-and-Language_Navigation_CVPR_2023_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/xiangyangli-cn/KERM.svg)](https://github.com/xiangyangli-cn/KERM) |---|
| 2023 | `MM` | [PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation](https://arxiv.org/pdf/2305.11918) |---|---|
| 2023 |`arXiv`| [CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation](https://arxiv.org/abs/2103.00852) |---|---|
| 2023 | `ACL` | [VLN-Trans: Translator for the Vision and Language Navigation Agent](https://arxiv.org/pdf/2302.09230) | [![Github stars](https://img.shields.io/github/stars/HLR/VLN-trans.svg)](https://github.com/HLR/VLN-trans) |---|
| 2022 | `ACL` | [Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration](https://arxiv.org/pdf/2203.04006) | [![Github stars](https://img.shields.io/github/stars/liangcici/Probes-VLN.svg)](https://github.com/liangcici/Probes-VLN) |---|
| 2022 | `CVPR` | [Less is More: Generating Grounded Navigation Instructions from Landmarks](https://arxiv.org/pdf/2004.14973) | [![Github stars](https://img.shields.io/github/stars/google-research-datasets/RxR.svg)](https://github.com/google-research-datasets/RxR/tree/main/marky-mT5) |---|
| 2022 | `MM` | [Target-Driven Structured Transformer Planner for Vision-Language Navigation](https://arxiv.org/pdf/2207.11201) | [![Github stars](https://img.shields.io/github/stars/YushengZhao/TD-STP.svg)](https://github.com/YushengZhao/TD-STP) |---|
| 2022 | `CVPR` | [HOP: History-and-Order Aware Pre-training for Vision-and-Language Navigation](https://ieeexplore.ieee.org/document/9880046) | [![Github stars](https://img.shields.io/github/stars/YanyuanQiao/HOP-VLN.svg)](https://github.com/YanyuanQiao/HOP-VLN) |---|
| 2022 | `International Conference on Computational Linguistics` | [LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation](https://aclanthology.org/2022.coling-1.505.pdf) | [![Github stars](https://img.shields.io/github/stars/HLR/LOViS.svg)](https://github.com/HLR/LOViS) |---|
| 2022 | NACCL | [Diagnosing Vision-and-Language Navigation: What Really Matters](https://aclanthology.org/2022.naacl-main.438.pdf) | [![Github stars](https://img.shields.io/github/stars/VegB/Diagnose_VLN.svg)](https://github.com/VegB/Diagnose_VLN) |---|
| 2022 |`arXiv`<br>University of Maryland| [CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/pdf/2211.16649) |---|é‡‡ç”¨å±€éƒ¨å¢é‡è§„åˆ’æ¡†æ¶ï¼Œå°†å¯¼èˆªè¿‡ç¨‹åˆ†è§£ä¸ºå•æ­¥å†³ç­–åºåˆ—(æŒ‡ä»¤åˆ†è§£)ï¼šé€šè¿‡CLIPå®æ—¶è®¡ç®—è§†è§‰åœºæ™¯ä¸å­æŒ‡ä»¤ï¼ˆå¦‚ã€Œæ‰¾å¨æˆ¿é—¨ã€ï¼‰çš„åŒ¹é…åº¦ï¼Œç›´æ¥é¢„æµ‹æœ€ä¼˜åŠ¨ä½œï¼ˆå·¦è½¬/ç›´è¡Œï¼‰|
| 2022 | `CVPR` | [Bridging the Gap Between Learning in Discrete and Continuous Environments for Vision-and-Language Navigation](https://arxiv.org/abs/2203.02764) | [![Github stars](https://img.shields.io/github/stars/YicongHong/Discrete-Continuous-VLN.svg)](https://github.com/YicongHong/Discrete-Continuous-VLN) |---|
| 2021 | `CVPR` | [Scene-Intuitive Agent for Remote Embodied Visual Grounding](https://arxiv.org/pdf/2103.12944) |---|---|
| 2021 | `NeurIPS` | [SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation](https://arxiv.org/abs/2110.14143) |---|---|
| 2021 | `ICCV` | [The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation](https://openaccess.thecvf.com/content/ICCV2021/papers/Qi_The_Road_To_Know-Where_An_Object-and-Room_Informed_Sequential_BERT_for_ICCV_2021_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YuankaiQi/ORIST.svg)](https://github.com/YuankaiQi/ORIST) |---|
| 2021 | `CVPR` | [VLN BERT: A Recurrent Vision-and-Language BERT for Navigation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf) | [![Github stars](https://img.shields.io/github/stars/YicongHong/Recurrent-VLN-BERT.svg)](https://github.com/YicongHong/Recurrent-VLN-BERT) |---|
| 2021 | EACL | [On the Evaluation of Vision-and-Language Navigation Instructions](https://arxiv.org/abs/2101.10504) |---|---|
|---|---| [Do As I Can, Not As I Say: Grounding Language in Robotic Affordances](https://say-can.github.io/assets/palm_saycan.pdf) | [![Github stars](https://img.shields.io/github/stars/say-can/say-can.github.io.svg)](https://say-can.github.io/) |---|
|2021|`NIPS`<br>PSL Research University|[History aware multimodal transformer for vision-and-language navigation](https://proceedings.neurips.cc/paper/2021/file/2e5c2cb8d13e8fba78d95211440ba326-Paper.pdf)|[![Github stars](https://img.shields.io/github/stars/cshizhe/VLN-HAMT.svg)](https://github.com/cshizhe/VLN-HAMT)<br>[website](https://cshizhe.github.io/projects/vln_hamt.html)|æå‡ºHistory Aware Multimodal Transformer (HAMT)ï¼Œé€šè¿‡transformerå°†é•¿æœŸå†å²çº³å…¥VLNå†³ç­–ä¸­;å…ˆé‡‡ç”¨ViTæ¥ç¼–ç æ¯å¼ ç‹¬ç«‹çš„å›¾åƒï¼Œç„¶åå°†å…¨æ™¯è§‚æµ‹ä¸æ¯å¼ å›¾åƒçš„ç©ºé—´å…³ç³»å»ºæ¨¡ï¼Œæœ€åæŠŠä¸åŒå…¨æ™¯å›¾çš„æ—¶é—´ç»´åº¦çš„å…³è”è€ƒè™‘åœ¨å†…|
|2021|`CVPR`|[Room-and-object aware knowledge reasoning for remote embodied referring expression](https://openaccess.thecvf.com/content/CVPR2021/papers/Gao_Room-and-Object_Aware_Knowledge_Reasoning_for_Remote_Embodied_Referring_Expression_CVPR_2021_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/alloldman/CKR.svg)](https://github.com/alloldman/CKR)|---|
|2021|`ICCV`<br>Chinese Academy of Sciences|[Vision-language navigation with random environmental mixup](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Vision-Language_Navigation_With_Random_Environmental_Mixup_ICCV_2021_paper.pdf)|---|é€šè¿‡æ··åˆæŒ‡ä»¤ä¸å¯¹åº”çš„è½¨è¿¹/ç¯å¢ƒï¼Œå®ç°VLNæ•°æ®å¢å¹¿|
|2021|`ICRA`<br>Georgia Institute of Technology |[Hierarchical cross-modal agent for robotics vision-and-language navigation](https://arxiv.org/pdf/2104.10674)|---|Robo-VLN <br> é¦–ç¯‡é’ˆå¯¹è¿ç»­3Dç©ºé—´çš„VLNä»»åŠ¡;é‡‡ç”¨çš„ç±»ä¼¼åŒç³»ç»Ÿæ¶æ„ï¼šhigh-level policyï¼šå°†è¾“å…¥çš„æ–‡æœ¬åŠè§†è§‰æ¨¡æ€è¿›è¡Œå¯¹é½åŠæ¨ç†ï¼Œå¹¶è¾“å‡ºsub-goalï¼›low-level policyï¼šå°†æ¥è‡ªé«˜å±‚ç­–ç•¥çš„sub-goal outputè½¬æ¢ä¸ºåº•å±‚actionï¼Œå¹¶ä¸”é€šè¿‡æ¨¡ä»¿å­¦ä¹ ç­–ç•¥æœ‰æ•ˆåœ°æ¨¡ä»¿ä¸“å®¶æ§åˆ¶å™¨ï¼›ç½‘ç»œæ¶æ„æ–¹é¢ï¼šè¯­è¨€æŒ‡ä»¤ç”±BERT embeddingï¼ŒRGBDæ•°æ®ç”±CNNæå–ç‰¹å¾ï¼Œé€šè¿‡Transformerå°†è§†è§‰ä¸æ–‡æœ¬ç‰¹å¾èåˆï¼Œå¯¹äºdecoderé‡‡ç”¨RNNï¼Œå¯¹äºlow-level actionç›´æ¥è¾“å‡ºagentçš„çº¿é€Ÿåº¦ä¸è§’é€Ÿåº¦| 
| 2020 | `CVPR`<br>Duke University | [Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training](https://arxiv.org/abs/2002.10638) | [![Github stars](https://img.shields.io/github/stars/weituo12321/PREVALENT.svg)](https://github.com/weituo12321/PREVALENT) |é¦–æ¬¡æå‡ºé€šè¿‡é¢„è®­ç»ƒä»¥åŠfine-tuningçš„èŒƒå¼æ¥å®ç°VLNä»»åŠ¡|
|2020|`ECCV`<br>Beijing Institute of Technology|[Active visual information gathering for vision-language navigation](https://arxiv.org/pdf/2007.08037)|[![Github stars](https://img.shields.io/github/stars/HanqingWangAI/Active_VLN.svg)](https://github.com/HanqingWangAI/Active_VLN)|å½“æŒ‡ä»¤æ¨¡ç³Šæˆ–è§‚æµ‹ä¸å¤Ÿå¯¼è‡´ä¸ç¡®å®šæ—¶ï¼Œæ‰§è¡Œactive explorationï¼›å…³é”®åœ¨äºï¼šå†³å®šä»€ä¹ˆæ—¶å€™å±•å¼€æ¢ç´¢ã€å“ªä¸€éƒ¨åˆ†å€¼å¾—æ¢ç´¢|
|2020|`CVPR`<br>|[Vision-language navigation with self-supervised auxiliary reasoning tasks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhu_Vision-Language_Navigation_With_Self-Supervised_Auxiliary_Reasoning_Tasks_CVPR_2020_paper.pdf)|---|ä»è¯­ä¹‰ä¿¡æ¯ä¸­è·å–å››ä¸ªé¢å¤–çš„è®­ç»ƒä¿¡å·ï¼šè¿‡å»çš„actionã€å¯¼èˆªçš„è¿›ç¨‹ã€ä¸‹ä¸€åŠ¨ä½œæ˜¯å¦ä¸æŒ‡ç¤ºç›¸åŒ¹é…ã€è½¨è¿¹çš„ä¸€è‡´æ€§ï¼›ç½‘ç»œæ¶æ„ä¸ºLSTM+Attention+CNN|
|2020|`ECCV`<br>Georgia Institute of Technology|[Improving vision-and-language navigation with image-text pairs from the web](https://arxiv.org/pdf/2004.14973)|---|VLN-BERT<br>é€šè¿‡ç½‘ç»œæŠ“å–çš„è§†è§‰å’Œè¯­è¨€è¯­æ–™åº“æ¥è®©æ¨¡å‹å­¦ä¼švisual groundingsï¼Œè¿›è€Œæå‡æ€§èƒ½ï¼›é‡‡ç”¨äº†ViLBERT+BERT|
| 2020 | `ECCV` | [Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/abs/2004.02857) | [![Github stars](https://img.shields.io/github/stars/jacobkrantz/VLN-CE.svg)](https://github.com/jacobkrantz/VLN-CE) |---|
|2019|`EMNLP`|[Robust navigation with language pretraining and stochastic sampling](https://arxiv.org/pdf/1909.02244)|[![Github stars](https://img.shields.io/github/stars/xjli/r2r_vln.svg)](https://github.com/xjli/r2r_vln)|---|
|2019|`CoRL`|[Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight](https://arxiv.org/pdf/1910.09664)|[![Github stars](https://img.shields.io/github/stars/lil-lab/drif.svg)](https://github.com/lil-lab/drif)|---|
|2018|`NIPS`<br> University of California|[Speaker-follower models for vision-and-language navigation](https://arxiv.org/pdf/1806.02724)|[![Github stars](https://img.shields.io/github/stars/ronghanghu/speaker_follower.svg)](https://github.com/ronghanghu/speaker_follower)<br>[website](https://ronghanghu.com/speaker_follower/)|å°†VLNä»»åŠ¡è§†ä¸ºè½¨è¿¹æœç´¢é—®é¢˜:æŒ‡ä»¤è§£ææ¨¡å—(followerå°†æŒ‡ä»¤æ˜ å°„åˆ°åŠ¨ä½œç©ºé—´)+æŒ‡ä»¤ç”Ÿæˆæ¨¡å—(speakerå°†åŠ¨ä½œåºåˆ—æ˜ å°„å›æŒ‡ä»¤),ä¸¤è€…å‡ä¸ºseq2seqæ¶æ„ï¼›speaker modelå¯ä»¥é€šè¿‡çœŸå€¼å¯¼èˆªè·¯çº¿ä¸æŒ‡ä»¤è¿›è¡Œè®­ç»ƒï¼›åœ¨followeræµ‹è¯•çš„æ—¶å€™ï¼Œfollowerä¼šç”Ÿæˆç»™å®šçš„æŒ‡ä»¤çš„æ½œåœ¨è·¯çº¿ã€‚è€Œspeakerå¯¹è¿™äº›è·¯çº¿è¿›è¡Œæ’åï¼Œé€‰æ‹©èƒ½å¤Ÿæ›´å¥½çš„è§£ææŒ‡ä»¤çš„ä¸€æ¡ï¼ˆç±»ä¼¼GANç½‘ç»œçš„æ€è·¯ï¼‰|
|2018|`RSS`|[Following high-level navigation instructions on a simulated quadcopter with imitation learning](https://arxiv.org/pdf/1806.00047)|[![Github stars](https://img.shields.io/github/stars/lil-lab/gsmn.svg)](https://github.com/lil-lab/gsmn)|---|
|2018|`CVPR`<br>Australian National University|[Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments](https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/peteanderson80/Matterport3DSimulator.svg)](https://github.com/peteanderson80/Matterport3DSimulator)<br>[website](https://bringmeaspoon.org/)|ç¬¬ä¸€ç¯‡æå‡ºVLNä»»åŠ¡çš„è®ºæ–‡(ç¦»æ•£)ï¼›æå‡ºR2Ræ•°æ®é›†ï¼ˆMatterport3Dï¼Œ2Wå¤šæ¡å¯¼èˆªæŒ‡ä»¤ï¼‰ï¼›ç½‘ç»œç»“æ„éƒ¨åˆ†ï¼šè¯­è¨€é‡‡ç”¨çš„æ˜¯åŸºäºLSTMçš„seqtoseq ç»“æ„å’Œæ³¨æ„æœºåˆ¶ï¼Œå›¾åƒåˆ™æ˜¯ç”¨çš„ImageNetä¸Šé¢„è®­ç»ƒçš„ResNet-152 CNNï¼›|

<br><br><br>

## Simulator and Dataset

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2026|`arXiv`|[Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters](https://arxiv.org/pdf/2602.06427)|---|BridgeNavDatasetï¼Œé¦–ä¸ªæˆ·å¤–åˆ°å®¤å†…å¯¼èˆªæ•°æ®é›†,åŒ…å«55Kè¡—é“è§†å›¾å›¾åƒã€100+å°æ—¶è§†é¢‘ã€55Kè½¨è¿¹-æŒ‡ä»¤å¯¹|
|2026|`AAAI`|[UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories](https://arxiv.org/pdf/2512.09607)|[![Github stars](https://img.shields.io/github/stars/CASIA-IVA-Lab/UrbanNav.svg)](https://github.com/CASIA-IVA-Lab/UrbanNav)|æ¶µç›– 1500 å°æ—¶çš„ç¬¬ä¸€äººç§°è§†é¢‘æ•°æ®ï¼ŒåŒ…å«è¶…è¿‡ 300 ä¸‡æ¡ä¸è½¨è¿¹å¯¹é½çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œè¦†ç›–å¤šæ ·å…‰ç…§æ¡ä»¶ä¸å¤©æ°”åœºæ™¯|
|2025|`ICML`|[Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents](https://arxiv.org/pdf/2502.09560)|[![Github stars](https://img.shields.io/github/stars/EmbodiedBench/EmbodiedBench.svg)](https://github.com/EmbodiedBench/EmbodiedBench)|[website](https://embodiedbench.github.io/)<br>[dataset](https://huggingface.co/EmbodiedBench)|
|2025|`arXiv`|[MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots](https://arxiv.org/pdf/2511.17889)|[![Github stars](https://img.shields.io/github/stars/AIGeeksGroup/MobileVLA-R1.svg)](https://github.com/AIGeeksGroup/MobileVLA-R1)|MobileVLA-CoTæ•°æ®é›†<br>[website](https://aigeeksgroup.github.io/MobileVLA-R1/)|
|2025|`arXiv`|[VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.17221)| [![Github stars](https://img.shields.io/github/stars/Qi-Zhangyang/GPT4Scene-and-VLN-R1.svg)](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1)|VLN-Ego datasetï¼ˆHabitat simulatorï¼‰<br>[website](https://vlnr1.github.io/)|
|2025|`arXiv`|[InternVLA-N1: An Open Dual-System Vision-Language Navigation Foundation Model with Learned Latent Plans](https://internrobotics.github.io/internvla-n1.github.io/static/pdfs/InternVLA_N1.pdf)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternNav.svg)](https://github.com/InternRobotics/InternNav) |[website](https://internrobotics.github.io/internvla-n1.github.io/)<br>InternData-N1 Dataset|
|2025|`arXiv`|[HA-VLN: A Benchmark for Human-Aware Navigation in Discreteâ€“Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard](https://arxiv.org/pdf/2503.14229)|[![Github stars](https://img.shields.io/github/stars/F1y1113/HA-VLN.svg)](https://github.com/F1y1113/HA-VLN)|[websit](https://ha-vln-project.vercel.app/)|
*|2024|`IROS`|[Hm3d-ovon: A dataset and benchmark for open-vocabulary object goal navigation](https://arxiv.org/pdf/2409.14296)|[![Github stars](https://img.shields.io/github/stars/naokiyokoyama/ovon.svg)](https://github.com/naokiyokoyama/ovon)|[website](https://naoki.io/portfolio/ovon)|
|2023|`ICCV`|[Learning vision-and-language navigation from youtube videos](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Learning_Vision-and-Language_Navigation_from_YouTube_Videos_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/JeremyLinky/YouTube-VLN.svg)](https://github.com/JeremyLinky/YouTube-VLN)|YouTube-VLN|
|2023|`ICCV`|[Aerialvln: Vision-and-language navigation for uavs](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_AerialVLN_Vision-and-Language_Navigation_for_UAVs_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/AirVLN/AirVLN.svg)](https://github.com/AirVLN/AirVLN)|AerialVLN|
|2023|`ICCV`|[Scaling data generation in vision-and-language navigation](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/wz0919/ScaleVLN.svg)](https://github.com/wz0919/ScaleVLN)|ScaleVLN|
|2022|`CVPR`|[Habitat-web: Learning embodied object-search strategies from human demonstrations at scale](https://openaccess.thecvf.com/content/CVPR2022/papers/Ramrakhya_Habitat-Web_Learning_Embodied_Object-Search_Strategies_From_Human_Demonstrations_at_Scale_CVPR_2022_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/Ram81/habitat-web.svg)](https://github.com/Ram81/habitat-web)|[website](https://ram81.github.io/projects/habitat-web)|
|2022|`CVPR`|[Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation](https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Bridging_the_Gap_Between_Learning_in_Discrete_and_Continuous_Environments_CVPR_2022_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/YicongHong/Discrete-Continuous-VLN.svg)](https://github.com/YicongHong/Discrete-Continuous-VLN)|R2R-CE|
|2021|`CVPR`|[Soon: Scenario oriented object navigation with graph-based exploration](https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_SOON_Scenario_Oriented_Object_Navigation_With_Graph-Based_Exploration_CVPR_2021_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/ZhuFengdaaa/SOON.svg)](https://github.com/ZhuFengdaaa/SOON)|SOON|
|2020|`arXiv`|[Alfred: A benchmark for interpreting grounded instructions for everyday tasks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Shridhar_ALFRED_A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_CVPR_2020_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/askforalfred/alfred.svg)](https://github.com/askforalfred/alfred) |ALFRED<br>[website](AskForALFRED.com)| 
|2020|`CVPR`|[REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments](https://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_REVERIE_Remote_Embodied_Visual_Referring_Expression_in_Real_Indoor_Environments_CVPR_2020_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/YuankaiQi/REVERIE.svg)](https://github.com/YuankaiQi/REVERIE)|REVERIE|
|2020|`EMNLP`|[Where are you? localization from embodied dialog](https://arxiv.org/pdf/2011.08277)|---|[website](https://meerahahn.github.io/way/)|
|2020|`CoRL`|[Vision-and-Dialog Navigation](https://arxiv.org/pdf/1907.04957)|[![Github stars](https://img.shields.io/github/stars/mmurray/cvdn.svg)](https://github.com/mmurray/cvdn/)|CVDN<br>[website](https://cvdn.dev/)|
|2020|`EMNLP`|[Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding](https://arxiv.org/pdf/2010.07954)|[![Github stars](https://img.shields.io/github/stars/google-research-datasets/RxR.svg)](https://github.com/google-research-datasets/RxR)|RxR|
|2019|`EMNLP`|[Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning](https://arxiv.org/pdf/1909.01871)|[![Github stars](https://img.shields.io/github/stars/khanhptnk/hanna.svg)](https://github.com/khanhptnk/hanna)|HANNA|
|2019|`ACL`|[Stay on the path: Instruction fidelity in vision-and-language navigation](https://arxiv.org/pdf/1905.12255)|---|R4R<br>[website](https://github.com/google-research/google-research/tree/master/r4r)|
|2019|`arXiv`|[Learning to navigate unseen environments: Back translation with environmental dropout](https://arxiv.org/pdf/1904.04195)|[![Github stars](https://img.shields.io/github/stars/airsplay/R2R-EnvDrop.svg)](https://github.com/airsplay/R2R-EnvDrop)|R2R-EnvDrop-CE|
|2018|`CVPR`|[Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments](https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/peteanderson80/Matterport3DSimulator.svg)](https://github.com/peteanderson80/Matterport3DSimulator)|R2R<br>[website](https://bringmeaspoon.org/)|


## Survey Paper
<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey](https://arxiv.org/pdf/2508.15354)|[![Github stars](https://img.shields.io/github/stars/Franky-X/Awesome-Embodied-Navigation.svg)](https://github.com/Franky-X/Awesome-Embodied-Navigation)|Survey for EN <br> [blog](https://kwanwaipang.github.io/Enbodied-Navigation/)|
|2025|`Transactions on Mechatronics`|[Aligning cyber space with physical world: A comprehensive survey on embodied ai](https://arxiv.org/pdf/2407.06886?)|[![Github stars](https://img.shields.io/github/stars/HCPLab-SYSU/Embodied_AI_Paper_List.svg)](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)|---|
|2024|`Transactions on Machine Learning Research`|[Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models](https://openreview.net/pdf?id=yiqeh2ZYUh)|[![Github stars](https://img.shields.io/github/stars/zhangyuejoslin/VLN-Survey-with-Foundation-Models.svg)](https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models)|[blog](https://kwanwaipang.github.io/VLNsurvery2024/)|
|2024|`arXiv`|[A Survey on Vision-Language-Action Models for Embodied AI](https://arxiv.org/pdf/2405.14093)|[![Github stars](https://img.shields.io/github/stars/yueen-ma/Awesome-VLA.svg)](https://github.com/yueen-ma/Awesome-VLA)|Survey for VLA|
|2024|`Neural Computing and Applications`|[Vision-language navigation: a survey and taxonomy](https://arxiv.org/pdf/2108.11544)|---|---|
|2023|`Artificial Intelligence Review`|[Visual language navigation: A survey and open challenges](https://link.springer.com/article/10.1007/s10462-022-10174-9)|---|---|
|2022|`ACL`|[Vision-and-language navigation: A survey of tasks, methods, and future directions](https://arxiv.org/pdf/2203.12667)|[![Github stars](https://img.shields.io/github/stars/eric-ai-lab/awesome-vision-language-navigation.svg)](https://github.com/eric-ai-lab/awesome-vision-language-navigation)|---|



<!-- 

<br>
| Simulator | Dataset | Link | Note |
|:---------:|:-------:| ---- |:----:|
|VizDooma|---|[website](https://vizdoom.cs.put.edu.pl/)|---|
|House3D|SUNCG|[website](https://github.com/facebookresearch/House3D)|---|
|AI2THOR|---|[website](http://ai2thor.allenai.org)|---|
|Gibson|2D-3D-S|[website](http://gibsonenv.stanford.edu/)|---|
|iGibson|iGibson|[website](http://gibsonenv.stanford.edu/)|---|
|Matterport3DSimulator|R2R, R4R, REVERIE, SOON|---|
|Habitat|VLN-CE|[website](https://aihabitat.org/)|---|
|AirSim|AerialVLN|[website](https://github.com/microsoft/AirSim)|---|

-->




# Learning-based Navigation
or Image-goal Navigation, or object-goal navigation
<!-- |---|`arXiv`<br>|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`<br>Beihang University|[RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation](https://arxiv.org/pdf/2512.24212)|---|å¯¹äºRGBè¾“å…¥å…ˆé€šè¿‡MASt3R-SLAMä¼°ç®—poseå’Œdense 3D mapï¼Œç„¶åè°ƒç”¨è¯­ä»¥æ¨¡å—ï¼ˆCLIP+Grounding DINOï¼‰ç”Ÿæˆè¯­ä¹‰ç‚¹äº‘ï¼Œå¯¼èˆªåˆ™æ˜¯é‡‡ç”¨åŸºäº2Dæ …æ ¼ï¼ˆç”±3DæŠ•å½±è€Œæ¥ï¼‰çš„è·¯å¾„è§„åˆ’ï¼Œè¿›è€Œå®ç°zero-shotç›®æ ‡å¯¼èˆª|
|2025|`arXiv`<br>Tsinghua University|[LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry](https://arxiv.org/pdf/2512.19629)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/NavDP.svg)](https://github.com/InternRobotics/NavDP)<br>[website](https://steinate.github.io/logoplanner.github.io/)|ä»…éœ€ç›¸æœºçš„ç«¯åˆ°ç«¯é—­ç¯å¯¼èˆªï¼›ViTæå–è§†è§’ç‰¹å¾å¹¶ä¸çœŸå®å°ºåº¦å¯¹é½ï¼Œé‡‡ç”¨é•¿åºåˆ—è§‚æµ‹åšéšå¼çŠ¶æ€ä¼°è®¡ï¼Œé€šè¿‡diffusionç”Ÿæˆè½¨è¿¹ï¼›è·¨æœ¬ä½“ï¼ˆè½®å¼ã€å››è¶³ã€äººå½¢ï¼‰|
|2025|`arXiv`<br>University of Groningen|[REASAN: Learning Reactive Safe Navigation for Legged Robots](https://arxiv.org/pdf/2512.09537)|[![Github stars](https://img.shields.io/github/stars/ASIG-X/REASAN.svg)](https://github.com/ASIG-X/REASAN)<br>[website](https://asig-x.github.io/reasan_web/)|ä¸‰ä¸ªRLç­–ç•¥ç½‘ç»œï¼ˆè¿åŠ¨æ§åˆ¶ã€å®‰å…¨é˜²æŠ¤å’Œå¯¼èˆªï¼‰+åŸºäº Transformer çš„å¤–æ„ŸçŸ¥ä¼°è®¡å™¨ï¼ˆLiDAR--->åŠ¨æ€éšœç¢ç‰©çš„æ„ŸçŸ¥/Ray-based representationï¼‰,é«˜è¾¾2.5m/sä¸3 rad/sçš„é€Ÿåº¦ï¼ŒUnitree Go2 ï¼ŒJetson AGX Orin|
|2025|`TRO`|[Neupan: Direct point robot navigation with end-to-end model-based learning](https://arxiv.org/pdf/2403.06828)| [![Github stars](https://img.shields.io/github/stars/hanruihua/NeuPAN.svg)](https://github.com/hanruihua/NeuPAN)<br> [![Github stars](https://img.shields.io/github/stars/hanruihua/neupan_ros.svg)](https://github.com/hanruihua/neupan_ros)<br> [![Github stars](https://img.shields.io/github/stars/KevinLADLee/neupan_ros2.svg)](https://github.com/KevinLADLee/neupan_ros2) |[website](https://hanruihua.github.io/neupan_project/)|
|2025|`arXiv`|[Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation](https://arxiv.org/pdf/2510.08713)|[![Github stars](https://img.shields.io/github/stars/F1y1113/UniWM.svg)](https://github.com/F1y1113/UniWM)|---|
|2025|`arXiv`|[NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance](https://arxiv.org/pdf/2505.08712)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/NavDP.svg)](https://github.com/InternRobotics/NavDP)|[website](https://wzcai99.github.io/navigation-diffusion-policy.github.io/)|
|2025|`arXiv`|[MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation](https://arxiv.org/pdf/2511.10376v2)|[![Github stars](https://img.shields.io/github/stars/ylwhxht/MSGNav.svg)](https://github.com/ylwhxht/MSGNav)|---|
|2025|`arXiv`|[Adaptive Interactive Navigation of Quadruped Robots using Large Language Models](https://arxiv.org/pdf/2503.22942?)|---|[Video](https://www.youtube.com/watch?v=W5ttPnSap2g)|
|2025|`arXiv`|[DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction](https://www.arxiv.org/pdf/2510.07152)|---|---|
|2025|`arXiv`|[IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/pdf/2508.00823)|[![Github stars](https://img.shields.io/github/stars/GWxuan/IGL-Nav.svg)](https://github.com/GWxuan/IGL-Nav)|[website](https://gwxuan.github.io/IGL-Nav/)<br>Exploration+target matching|
|2025|`arXiv`|[LOVON: Legged Open-Vocabulary Object Navigator](https://arxiv.org/pdf/2507.06747?)|[![Github stars](https://img.shields.io/github/stars/DaojiePENG/LOVON.svg)](https://github.com/DaojiePENG/LOVON)|[website](https://daojiepeng.github.io/LOVON/)|
|2025|`ICRA`|[TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals](https://arxiv.org/pdf/2509.08699)|[![Github stars](https://img.shields.io/github/stars/podgorki/TANGO.svg)](https://github.com/podgorki/TANGO)|[website](https://podgorki.github.io/TANGO/)|
|2025|`RSS`|[Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation](https://arxiv.org/pdf/2504.19322)|[![Github stars](https://img.shields.io/github/stars/leggedrobotics/fdm.svg)](https://github.com/leggedrobotics/fdm)|[website](https://leggedrobotics.github.io/fdm.github.io/)|
|2025|`arXiv`|[Parkour in the Wild: Learning a General and Extensible Agile Locomotion Policy Using Multi-expert Distillation and RL Fine-tuning](https://arxiv.org/pdf/2505.11164)|---|---|
|2025|`CoRL`|[Omni-Perception: Omnidirectional Collision Avoidance for Legged Locomotion in Dynamic Environments](https://arxiv.org/pdf/2505.19214)|[![Github stars](https://img.shields.io/github/stars/aCodeDog/OmniPerception.svg)](https://github.com/aCodeDog/OmniPerception)|---|
|2024|`ICRA`|[VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation](https://arxiv.org/pdf/2312.03275)|[![Github stars](https://img.shields.io/github/stars/bdaiinstitute/vlfm.svg)](https://github.com/bdaiinstitute/vlfm)|[website](https://naoki.io/portfolio/vlfm)|
|2024|`SRO`|[Learning Robust Autonomous Navigation and Locomotion for Wheeled-Legged Robots](https://arxiv.org/pdf/2405.01792)|---|---| 
|2024|`RAL`|[PIE: Parkour With Implicit-Explicit Learning Framework for Legged Robots](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678805)|---|---| 
|2024|`ICRA`|[Extreme Parkour with Legged Robots](https://arxiv.org/pdf/2309.14341)|[![Github stars](https://img.shields.io/github/stars/chengxuxin/extreme-parkour.svg)](https://github.com/chengxuxin/extreme-parkour)|[website](https://extreme-parkour.github.io/)| 
|2023|`ICML`|[Esc: Exploration with soft commonsense constraints for zero-shot object navigation](https://proceedings.mlr.press/v202/zhou23r/zhou23r.pdf)|---|---|
|2023|`ICRA`|[Zero-shot object goal visual navigation](https://arxiv.org/pdf/2206.07423)|[![Github stars](https://img.shields.io/github/stars/pioneer-innovation/Zero-Shot-Object-Navigation.svg)](https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation)|---|
|2023|`ICRA`|[ViNL: Visual Navigation and Locomotion Over Obstacles](https://arxiv.org/pdf/2210.14791)|[![Github stars](https://img.shields.io/github/stars/SimarKareer/ViNL.svg)](https://github.com/SimarKareer/ViNL)|[website](https://www.joannetruong.com/projects/vinl.html)|
|2023|`Field Robotics`|[ArtPlanner: Robust Legged Robot Navigation in the Field](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10876046)|[![Github stars](https://img.shields.io/github/stars/leggedrobotics/art_planner.svg)](https://github.com/leggedrobotics/art_planner)|---|


## Mapless navigation
<!-- |---|`arXiv`<br>|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:| ----------- |
|2025|`arXiv`<br>TUM|[Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?](https://arxiv.org/pdf/2511.17792)|[![Github stars](https://img.shields.io/github/stars/TUM-AVS/target-bench.svg)](https://github.com/TUM-AVS/target-bench)<br> [website](https://target-bench.github.io/)<br> [Dataset](https://huggingface.co/target-bench)|æ•°æ®é›†Target-Bench: 450 ä¸ªè§†é¢‘ï¼ˆ112,500 å¸§ï¼‰ï¼Œè¦†ç›– 45 ç§è¯­ä¹‰ç›®æ ‡ç±»åˆ«ï¼Œæ¶‰åŠå¤šç§å®¤å†…å¤–ç¯å¢ƒã€‚æ•°æ®é›†åŒ…å«SLAMè½¨è¿¹ã€äººç±»æ ‡æ³¨çš„æ˜¾å¼å’Œéšå¼ç›®æ ‡ï¼Œæ•°æ®é›†å¹³å°ä¸ºå››è¶³æœºå™¨äººï¼ˆAGX Orin+Livox+åŒç›®ï¼‰; é¦–æ¬¡æå‡ºä¸–ç•Œæ¨¡å‹åœ¨æ— å›¾è·¯å¾„è§„åˆ’ä¸­çš„è¯„ä»·æ¡†æ¶ï¼ˆè·¯å¾„è¯„ä¼°æ¨¡å—ï¼‰ï¼›ä¸–ç•Œè§£ç å™¨(world decoder)ï¼šæ—¶ç©ºé‡å»ºæ¢å¤è½¨è¿¹ï¼ˆVGGT/SpaTracker/ViPEï¼‰+å°ºåº¦æ¢å¤ï¼ˆViPEä¸ºè§†è§‰æƒ¯æ€§SLAMä¸éœ€è¦ï¼Œå…¶ä»–ä¸¤è€…éœ€è¦çœŸå€¼å¯¹é½ï¼‰;è¯„ä¼°çš„ä¸–ç•Œæ¨¡å‹åŒ…æ‹¬äº†Soraã€Veoã€Wanä¸‰ä¸ªç³»åˆ—åŠå…¶å˜ä½“|
|2025|`RSS`|[CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance](https://arxiv.org/pdf/2503.03921?)|[![Github stars](https://img.shields.io/github/stars/ut-amrl/creste_public.svg)](https://github.com/ut-amrl/creste_public)<br> [website](https://amrl.cs.utexas.edu/creste/)|---|


# Others
<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`IEEE/ASME Transactions on Mechatronics`|[Aligning cyber space with physical world: A comprehensive survey on embodied ai](https://arxiv.org/pdf/2407.06886)|[![Github stars](https://img.shields.io/github/stars/HCPLab-SYSU/Embodied_AI_Paper_List.svg)](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)|---|
|2025|`arXiv`|[HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots](https://arxiv.org/pdf/2503.09010)|---|---|
|2021|`ICML`|[Learning transferable visual models from natural language supervision](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf)|[![Github stars](https://img.shields.io/github/stars/OpenAI/CLIP.svg)](https://github.com/OpenAI/CLIP)|CLIP<br>[website](https://openai.com/index/clip/)|

## Occupancy Perception
<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots](https://arxiv.org/pdf/2507.20217)|[![Github stars](https://img.shields.io/github/stars/Open-X-Humanoid/Humanoid-Occupancy.svg)](https://github.com/Open-X-Humanoid/Humanoid-Occupancy)|[website](https://humanoid-occupancy.github.io/)<br>Multimodal Occupancy Perception| 
|2025|`arXiv`|[Roboocc: Enhancing the geometric and semantic scene understanding for robots](https://arxiv.org/pdf/2504.14604)|---|3DGS|
|2025|`ICCV`|[EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding](https://arxiv.org/pdf/2412.04380)|[![Github stars](https://img.shields.io/github/stars/YkiWu/EmbodiedOcc.svg)](https://github.com/YkiWu/EmbodiedOcc)|[website](https://ykiwu.github.io/EmbodiedOcc/)|
|2023|`ICCV`|[Scene as occupancy](https://openaccess.thecvf.com/content/ICCV2023/papers/Tong_Scene_as_Occupancy_ICCV_2023_paper.pdf)|[![Github stars](https://img.shields.io/github/stars/OpenDriveLab/OccNet.svg)](https://github.com/OpenDriveLab/OccNet)|[Challenge and dataset](https://github.com/OpenDriveLab/OpenScene)|




